[["index.html", "fishLengthAssess R package 1 What is fishLengthAssess? 1.1 Installation 1.2 How to use fishLengthAssess?", " fishLengthAssess R package William Harford 2025-06-03 1 What is fishLengthAssess? fishLengthAssess acts as an aggregator of length-based stock assessment models. The package imposes a standardized formatting and metadata structure on length data sets, which can then be used in a variety of length-based assessment methods that are aggregated within the package. It also provides wrapper functions to link standardized formatting of length data sets with external R packages that contain length-based assessment methods (e.g. LBSPR package). 1.1 Installation fishLengthAssess depends on other packages to run its functions. These are the devtools, fishSimGTG (Harford 2024) and LBSPR (A. Hordyk 2021) R packages, that should be installed as follows: install.packages(&quot;devtools&quot;) install.packages(&quot;LBSPR&quot;) devtools::install_github(&quot;natureanalytics-ca/fishSimGTG@v1.0.6&quot;) The user then can install the development version of fishLengthAssess from GitHub with: devtools::install_github(&quot;natureanalytics-ca/fishLengthAssess&quot;) 1.2 How to use fishLengthAssess? The fishLengthAssess package aggregates data-limited methods that can be used for evaluating the condition of a fish stock based on length data sets. The methods are split in two categories: indicator-based functions and model-based functions. Indicator-based functions are based on metrics that include compliance with established size limits, 3 metrics proposed by Froese (2004), and trend-based metrics like mean length in the catch. Model-based methods consist of length-based stock assessment approaches, and includes an interface for estimation of SPR from observed length-frequency data, based on the LB-SPR method developed by Adrian R. Hordyk et al. (2016a). The main functions in the package rely on life histories and length composition objects (LifeHistoryObj and LengthCompObj, respectively). Once a user standardizes their length data to fit the requirements of the length composition object and gathers the necessary information for creating a life history object, then all the functions within the fishLengthAssess package are straightforward to use having these objects as function arguments. The next chapter describes in more detail how to structure the data into the necessary R objects. References Froese, Rainer. 2004. “Keep It Simple: Three Indicators to Deal with Overfishing.” Fish and Fisheries 5 (1): 86–91. https://doi.org/10.1111/j.1467-2979.2004.00144.x. Harford, W. 2024. “fishSimGTG: Numerical Simulations of Fish Population Dynamics. R Package Version 1.0.6.” https://github.com/natureanalytics-ca/fishSimGTG. Hordyk, A. 2021. “LBSPR: Length-Based Spawning Potential Ratio. R Package Version 0.1.6.” https://github.com/AdrianHordyk/LBSPR. Hordyk, Adrian R., Kotaro Ono, Jeremy D. Prince, and Carl J. Walters. 2016a. “A Simple Length-Structured Model Based on Life History Ratios and Incorporating Size-Dependent Selectivity: Application to Spawning Potential Ratios for Data-Poor Stocks.” Canadian Journal of Fisheries and Aquatic Sciences 73 (12): 1787–99. https://doi.org/10.1139/cjfas-2015-0422. "],["r-objects-and-example-data-sets.html", "2 R objects and example data sets", " 2 R objects and example data sets The main functions in the package rely on life histories and length composition objects. The life history object LifeHistoryObj is an S4 object that holds the description of a life history. It gathers information about life history traits of each species such as the parameters of the length-weight relationship, von Bertalanffy growth parameters, length at 50% maturity, natural mortality, and others. The LifeHistoryObj belongs to the class LifeHistory in the fishSimGTG R package. An example LifeHistoryObj can be accessed within the fishLengthAssess package (LifeHistoryExample) and the user can see the elements or slots of the LifeHistoryObj using the slotNames() function. # Load the package library(fishLengthAssess) # use function slotNames to see elements of LifeHistoryObj example slotNames(fishSimGTG::LifeHistoryExample) ## [1] &quot;title&quot; &quot;speciesName&quot; &quot;shortDescription&quot; ## [4] &quot;L_type&quot; &quot;L_units&quot; &quot;Walpha_units&quot; ## [7] &quot;Linf&quot; &quot;K&quot; &quot;t0&quot; ## [10] &quot;L50&quot; &quot;L95delta&quot; &quot;M&quot; ## [13] &quot;MK&quot; &quot;LW_A&quot; &quot;LW_B&quot; ## [16] &quot;Tmax&quot; &quot;Steep&quot; &quot;R0&quot; ## [19] &quot;recSD&quot; &quot;recRho&quot; &quot;isHermaph&quot; ## [22] &quot;H50&quot; &quot;H95delta&quot; # check what LifeHistoryExample contains fishSimGTG::LifeHistoryExample ## An object of class &quot;LifeHistory&quot; ## Slot &quot;title&quot;: ## [1] &quot;Example fish&quot; ## ## Slot &quot;speciesName&quot;: ## [1] &quot;Example fish&quot; ## ## Slot &quot;shortDescription&quot;: ## [1] &quot;Simulated life history of a fish based on B-H invariants&quot; ## ## Slot &quot;L_type&quot;: ## [1] &quot;TL&quot; ## ## Slot &quot;L_units&quot;: ## [1] &quot;cm&quot; ## ## Slot &quot;Walpha_units&quot;: ## [1] &quot;g&quot; ## ## Slot &quot;Linf&quot;: ## [1] 100 ## ## Slot &quot;K&quot;: ## [1] 0.2 ## ## Slot &quot;t0&quot;: ## [1] 0 ## ## Slot &quot;L50&quot;: ## [1] 66 ## ## Slot &quot;L95delta&quot;: ## [1] 1 ## ## Slot &quot;M&quot;: ## [1] 0.3 ## ## Slot &quot;MK&quot;: ## [1] 1.5 ## ## Slot &quot;LW_A&quot;: ## [1] 0.01 ## ## Slot &quot;LW_B&quot;: ## [1] 3 ## ## Slot &quot;Tmax&quot;: ## [1] 15 ## ## Slot &quot;Steep&quot;: ## [1] 0.99 ## ## Slot &quot;R0&quot;: ## [1] 1000 ## ## Slot &quot;recSD&quot;: ## [1] 0.6 ## ## Slot &quot;recRho&quot;: ## [1] 0 ## ## Slot &quot;isHermaph&quot;: ## [1] FALSE ## ## Slot &quot;H50&quot;: ## numeric(0) ## ## Slot &quot;H95delta&quot;: ## numeric(0) The values of the slots can be edited by the user using the @ symbol. # store LifeHistoryExample as a variable LifeHistory_example &lt;- fishSimGTG::LifeHistoryExample # change slot speciesName LifeHistory_example@speciesName &lt;- &quot;Example species&quot; LifeHistory_example@speciesName ## [1] &quot;Example species&quot; More details on the life history object can be found in the user guide of the fishSimGTG R package in here. The length composition object is a length data set that can be structured either as raw data (i.e. individual length measurements) or length frequency data (i.e. numbers of fish per length bin). The raw length data is a collection of length measurements stored as a vector. These are typically original length measurements of fish, which have not been binned. Multiple columns can be used, with each column pertaining to a level of a grouping variable, such as year or fleet. The length frequency data is a collection of length measurements organized using two or more columns. The left-most column must contain bin mid points. The next column contains the number of length measurement observations in each bin. While the first column is reserved for the bin mid points, multiple columns to its right can be used with each pertaining to a level of a grouping variable, such as year or fleet. # Example data set with length composition defined as length frequencies by year knitr::kable(LengthCompExampleFreq@dt[21:30,],row.names = F) LMids 2015 2016 2017 2018 2019 51 0 0 1 16 31 53 0 2 2 22 37 55 3 0 1 40 25 57 2 3 1 34 20 59 1 8 5 41 14 61 5 9 7 42 11 63 7 8 10 33 10 65 11 12 24 30 5 67 12 13 25 12 6 69 13 26 25 18 3 # Example data set with length composition defined as raw length measurements by year knitr::kable(head(LengthCompExampleLength@dt)) 2015 2016 2017 2018 2019 55 49 51 43 29 55 53 53 43 31 55 53 53 45 31 57 57 55 45 33 57 57 57 45 33 59 57 59 47 33 "],["indicator-based-functions.html", "3 Indicator-based functions 3.1 PmatFunc function 3.2 PoptFunc function 3.3 PmegaFunc function 3.4 PLcFunc function", " 3 Indicator-based functions 3.1 PmatFunc function The PmatFunc function calculates the proportion of the catch that is above length at 50% maturity (L50). This metric is calculated relative to the length at which 50% of fish are mature, which serves as a threshold to determine maturity. The goal for mature fish in the catch is 100%, as it is generally recommended to allow fish to reproduce at least once before becoming vulnerable to fishing (Froese 2004). Mature fish in the catch is calculated as the percentage of the length composition that is equal to or greater than L50. 3.2 PoptFunc function The PoptFunc function calculates the proportion of fish within optimal sizes in the catch. Optimal size refers to the length at which a fish cohort achieves its maximum biomass, and thus, fishing sizes close to this optimal size should ensure high yields by weight. This function relies on a utility function LoptFunc that calculates the optimal length in the catch (optimum harvest length; Beverton (1992)). The range of optimal lengths in the catch is then defined as \\(Lopt\\) +/- 10%. The percent optimal sizes in the catch is calculated as the percentage of the length composition that fall within the defined range. 3.3 PmegaFunc function The PmegaFunc function calculates the proportion of mega-spawners in the catch. Mega-spawners are those larger than the optimal size range, thus they should comprise a low percentage of the catch. Percent mega-spawners is calculated as the percentage of the length composition equal or greater than optimum length plus 10%. Interpretation of this metric is nuanced because while it may be desirable to leave mega-spawners in the water, if mega-spawners are not present in the catch but are not being intentionally avoided by fishers, a lack of mega-spawners could be an indication of overfishing. For this reason, Froese (2004) suggests a target of 30% to 40% of mega-spawners in the catch. Conversely, if mega-spawners are being avoided due to market preferences, gear selectivity or through other intentional interventions, such as slot limits, this can allow larger-older fish spawn and contribute to population replenishment. 3.4 PLcFunc function The PLcFunc function calculates the proportion of catch above the minimum size limit. For fisheries with size limits, fish above the size limit is calculated as the percentage of the length composition that are equal or greater than the size limit. References Beverton, R. J. H. 1992. “Patterns of Reproductive Strategy Parameters in Some Marine Teleost Fishes.” Journal of Fish Biology 41 (sB): 137–60. https://doi.org/10.1111/j.1095-8649.1992.tb03875.x. Froese, Rainer. 2004. “Keep It Simple: Three Indicators to Deal with Overfishing.” Fish and Fisheries 5 (1): 86–91. https://doi.org/10.1111/j.1467-2979.2004.00144.x. "],["model-based-functions.html", "4 Model-based functions 4.1 lbsprWrapper function 4.2 GTG Length-Based SPR model including dome-shaped selectivity", " 4 Model-based functions 4.1 lbsprWrapper function The lbsprWrapper function is a wrapper function for conducting length-based stock assessment (LBSPR) using length-based SPR methods (Hordyk et al. 2016). The model can use a conventional age-structured equilibrium population model or a length-structured version that is determined by growth-type-groups to account for size-based selectivity. Length frequency distribution of the stock is determined by natural mortality, fishing mortality, and length-based (LB) vulnerability to fishing. A relationship also exists between reproductive output of the population and length frequency distribution. Accordingly, measurements of the length frequency distribution can be used to infer SPR. The maximum likelihood LB-SPR estimation routine requires inputs of M/K, asymptotic length, a logistic maturity curve, coefficient of variation of asymptotic length and exponential parameter for fecundity. The former three inputs are obtained from a life history. The latter two inputs are assumed as follows: coefficient of variation of asymptotic length is specified at its default value of 0.1 and exponential parameter for fecundity is set equal to the beta parameter of the length-weight relationship or to a value of 3.0 if the beta parameter is unavailable. A length data set is also required, with the following caveats. First, LBSPR bins length data to produce a length-frequency distribution. When a length data set contains raw lengths, a bin width of 1 cm is used. When a length data set contains frequencies, the bin width of the length data set is used in the LBSPR fitting routine. Second, because LBSPR estimates the parameter of logistic selectivity function during the fitting process, the gear used to collect the length data must have asymptotic selectivity. Do not use LBSPR is this assumption cannot be met. Third, LBSPR assumes size structure data are representative at face value, so instances of hyperstability in the length frequency should be treated with caution (Coscino et al. 2024). For more information on the LBSPR model the user can check the package vignette here. 4.2 GTG Length-Based SPR model including dome-shaped selectivity This code is based on the GTG LBSPR framework originally developed by Adrian R. Hordyk et al. (2016b), which implements a growth-type-group (GTG) approach to simulate length-based per-recruit dynamics (https://github.com/AdrianHordyk/GTG_LBSPR). The GTG LBSPR code was extended by Hommik et al. (2020) (https://github.com/KHommik/DomeShaped_GTG_LBSPR) to incorporate dome-shaped selectivity models, specifically normal and log-normal curves, allowing more realistic selectivity patterns for length-based simulations in data-limited contexts. This current version further expands the functionality by integrating selectivity models commonly used in gillnet fisheries, particularly those available in the TropFishR package (Mildenberger, Taylor, and Wolff 2017), including: Normal (common spread) model, Normal (scaled spread) model, and Lognormal model. These three selectivity models are based on the SELECT framework developed by Millar and Holst (1997), which uses log-linear models for fitting gillnet and hook selectivity. Additionally, the code now supports two bimodal selectivity options: bimodal normal with scaled spread and bilognorm — bimodal log-normal. The new configurations allow that the dome-shaped models can be used either with aggregated mesh selectivity (as in Hommik et al. (2020)) or with individual mesh-specific (specific mesh size) selectivity. In addition the new code accepts different groups of length data providing the user the opportunity to estimate F/M and SPR for each group or for the pooled data. This code implements a length-based assessment approach that accounts for individual variation in growth using the Growth-Type Group (GTG) approach; supports multiple selectivity curves, including dome-shaped options; provides parameter estimation via maximum likelihood; and calculates key fisheries reference points like SPR. The model is part of the R package fishLengthAssess and and consists of several interconnected functions: GTGDomeLBSPRSim2(): The core simulation function that calculates SPR based on life history parameters, fleet parameters, and size bins. This function contains all the biological and mathematical models. processLengthCompData(): Data preprocessing function that converts S4 LengthComp objects into the format required by the optimization functions. Handles both frequency and raw length data with options for grouped or pooled analysis. OptFunDome: The objective function used during parameter estimation. This function takes trial parameter values, runs the simulation (GTGDomeLBSPRSim2()), compares predicted vs observed length compositions, and returns a negative log-likelihood value. DoOptDome: The main optimization function that manages the parameter estimation process (uses optim()). It sets up initial parameter values, calls the optimization algorithm (which repeatedly uses OptFunDome() to evaluate different parameter combinations), and processes the final results including standard errors and model diagnostics. DoOptDome.LengthComp(): It is a user interface function that accepts S4 LengthComp objects and automatically handles data processing before calling the core optimization functions. It provides options for analyzing multiple groups separately (byGroup = TRUE) or combining them into a single analysis (byGroup = FALSE). DoOptDome.aggregated(): It is a user interface function that ensures data from multiple groups is always combined (pooled) before analysis, regardless of the original data structure. This is useful when the user wants to analyze the combined dataset. run_grouped_and_pooled(): This function provides a comparative analysis that automatically runs the same dataset through both grouped analysis (each group fitted separately) and pooled analysis (all groups combined), allowing users to compare results and assess whether pooling affects parameter estimates. 4.2.1 Core simulation function: GTGDomeLBSPRSim2 This is the main function of the model. GTGDomeLBSPRSim2() simulates per-recruit population dynamics for fish populations structured by Growth-Type Groups (GTGs), allowing for dome-shaped or logistic selectivity curves. It supports mesh-specific or aggregated selectivity and returns biological reference points such as spawning potential ratio (SPR), yield per recruit (YPR), and equilibrium recruitment. GTGDomeLBSPRSim2 &lt;- function(lifeHistoryObj, FleetPars, SizeBins=NULL) 4.2.1.1 Arguments The arguments of the GTGDomeLBSPRSim2() function are: lifeHistoryObj: S4 life history object containing biological parameters (growth, maturity, etc.). FleetPars: List containing fishery parameters (selectivity, fishing mortality). SizeBins: List defining length bins for analysis. lifeHistoryObj(S4 Object): The life history object (lifeHistoryObj) contains the following slots accessed via @: @Linf: Mean asymptotic length @MK: M/K ratio (natural mortality over growth) @L50: Length at 50% maturity @L95delta: Delta between L95 and L50 maturity (L95 = L50 + L95delta) @LW_A, @LW_B: Weight-at-length coefficients (Walpha, Wbeta) @Steep: Beverton-Holt steepness @R0: Virgin recruitment (can be a large arbitrary number, e.g., 1e6) Additional attributes are added to this lifeHistoryObj via attr(): NGTG: Number of growth-type groups (default = 13) CVLinf: Coefficient of variation in Linf (default = 0.1) MaxSD: Multiplier of SD to define GTG range (default = 2) GTGLinfBy: Increment between GTG Linf values (default = NA) FecB: Fecundity exponent (default = 3) Mpow: Exponent for M/K ratio (default = 0) FleetPars: Required elements: FM: Fishing mortality rate (F/M) selectivityCurve: One of the following supported selectivity curves \"Logistic\": Two-parameter logistic selectivity (SL1: size at length of 50% of selectivity and SL2: size at length of 95% of selectivity). \"Knife\": Binary selectivity above a threshold length \"Normal.loc\": Normal (common spread) model \"Normal.sca\": Normal (scaled spread) model \"lognorm\": Lognormal model \"binorm.sca\": Bi-normal model \"bilognorm\": Bi-lognormal model A complete description of dome-shaped selectivity curves is provided in Chapter 5, section 5.4 Selectivity specific parameters: SL1-SL5: Selectivity parameters (see table below) SLmesh: Vector of mesh sizes (for dome-shaped models) SLMin: (Optional) Minimum length selected MLLKnife: Minimum legal length (for “Knife” selectivity) use_aggregated: If TRUE, aggregate selectivity across mesh sizes fishery_mesh: (Optional) mesh size used if not aggregated Selectivity Type SL1 SL2 SL3 SL4 SL5 \"Logistic\" Length at 50% selectivity Length at 95% selectivity — — — \"Normal.loc\" Mode of normal curve Spread (SD) (fixed) — — — \"Normal.sca\" Mode of normal curve Spread (SD) (proportional to mesh) — — — \"logNorm\" Parameter mean in log space Parameter SD in log space — — — \"binorm.sca\" Mode 1 (first peak) SD 1 Mode 2 (second peak) SD 2 Logit(P1): proportion of retention assigned to the first peak \"bilognorm\" Mode 1 (first peak in log space) SD 1 in log space Mode 2 (second peak in log space) SD 2 in log space Logit(P1): proportion of retention assigned to the first peak \"Knife\" — — — — — Note: In bimodal models, SL5 is a logit-transformed parameter that represents the proportion of retention assigned to the first peak. \\[ P_1 = \\frac{\\exp(\\text{SL5})}{1 + \\exp(\\text{SL5})} \\] A complete description of dome-shaped selectivity parameters is provided in Section 5: Utility functions. SizeBins: Required elements: Linc: Bin width (default = 1) ToSize: Upper bound for length classes (default = Linf + MaxSD * SD_Linf) 4.2.1.2 Parameter extraction and setup of GTGDomeLBSPRSim2() # Direct S4 access to lifeHistoryObj NGTG &lt;- attr(lifeHistoryObj, &quot;NGTG&quot;) if (is.null(NGTG)) NGTG &lt;- 13 # Default Linf &lt;- lifeHistoryObj@Linf MK &lt;- lifeHistoryObj@MK L50 &lt;- lifeHistoryObj@L50 L95 &lt;- lifeHistoryObj@L50 + lifeHistoryObj@L95delta # Convert from delta # ... more parameters extracted The function first extracts biological parameters from the S4 life history object lifeHistoryObj, including: Growth parameters (Linf, CV) via direct slot access (@) and attribute access (attr()) Mortality-to-growth ratio (M/K) Maturity parameters (L50, L95delta) Weight-length relationship (LW_A, LW_B) Fecundity parameters 4.2.1.3 Setting up Growth-Type Groups (GTGs) The function creates a range of Linf values centered on the mean Linf and distributes recruits across these groups. This accounts for individual variability in growth. # Set up Linfs for different GTGs if (exists(&quot;NGTG&quot;) &amp; !exists(&quot;GTGLinfBy&quot;)) { DiffLinfs &lt;- seq(from=Linf-MaxSD*SDLinf, to=Linf+MaxSD*SDLinf, length=NGTG) GTGLinfBy &lt;- DiffLinfs[2]-DiffLinfs[1] } else if (!exists(&quot;NGTG&quot;) &amp; exists(&quot;GTGLinfBy&quot;)) { DiffLinfs &lt;- seq(from=Linf-MaxSD*SDLinf, to=Linf+MaxSD*SDLinf, by=GTGLinfBy) NGTG &lt;- length(DiffLinfs) } else if (exists(&quot;NGTG&quot;) &amp; exists(&quot;GTGLinfBy&quot;)) { if (!is.na(GTGLinfBy)) { DiffLinfs &lt;- seq(from=Linf-MaxSD*SDLinf, to=Linf+MaxSD*SDLinf, by=GTGLinfBy) NGTG &lt;- length(DiffLinfs) } if (is.na(GTGLinfBy)) { DiffLinfs &lt;- seq(from=Linf-MaxSD*SDLinf, to=Linf+MaxSD*SDLinf, length=NGTG) GTGLinfBy &lt;- DiffLinfs[2]-DiffLinfs[1] } } 4.2.1.4 Selectivity model implementation A major feature of this model is its ability to use different selectivity curves: if(selectivityCurve==&quot;Logistic&quot;){ VulLen &lt;- 1.0/(1+exp(-log(19)*((LenBins+0.5*Linc)-SL50)/((SL95)-(SL50)))) } else if(selectivityCurve==&quot;Normal.sca&quot;){ # Normal Scale implementation # ... } else if(selectivityCurve==&quot;Normal.loc&quot;){ # Normal Location implementation # ... } # ... more selectivity options The model supports both mesh-aggregated and single-mesh selectivity for dome-shaped curves. 4.2.1.5 Population dynamics simulation The function simulates both unfished and fished populations, tracking numbers, biomass, and reproductive output across length classes and growth-type groups. # Initialize matrices NPRFished &lt;- NPRUnfished &lt;- matrix(0, nrow=length(LenBins), ncol=NGTG) # ... more matrices # Distribute recruits to first length class NPRFished[1, ] &lt;- NPRUnfished[1, ] &lt;- RecProbs * R0 # Calculate numbers at each size class for (L in 2:length(LenBins)) { NPRUnfished[L, ] &lt;- NPRUnfished[L-1, ] * ((DiffLinfs-LenBins[L])/(DiffLinfs-LenBins[L-1]))^MKMat[L-1, ] NPRFished[L, ] &lt;- NPRFished[L-1, ] * ((DiffLinfs-LenBins[L])/(DiffLinfs-LenBins[L-1]))^ZKLMat[L-1, ] # ... more calculations } 4.2.1.6 SPR and Yield calculation The function calculates SPR as the ratio of eggs-per-recruit in the fished vs. unfished population, as well as yield-per-recruit and total yield. # Calculate SPR EPR0 &lt;- sum(NatLUnFishedPop * FecLenGTG) # Eggs-per-recruit Unfished EPRf &lt;- sum(NatLFishedPop * FecLenGTG) # Eggs-per-recruit Fished SPR &lt;- EPRf/EPR0 # Spawning potential ratio # Calculate yield YPR &lt;- sum(NatLFishedPop * Weight * VulLen2) * FM Yield &lt;- YPR * RelRec 4.2.1.7 Main outputs of the GTGDomeLBSPRSim2() function The function returns a comprehensive list of calculated values, including: Element Description SPR Spawning Potential Ratio YPR, Yield Yield per recruit and total yield LCatchFished, LCatchUnfished Normalized length composition of catch (fished and unfished) LPopFished, LPopUnfished Normalized population-at-length (fished and unfished) NatLPopFished, NatLPopUnFish Raw GTG-specific number-at-length matrices (fished and unfished) NatLCatchFish, NatLCatchUnFish GTG-specific catch matrices (fish and unfished) FecLen, MatLen Fecundity and maturity-at-length per GTG SelLen, VulLen2 Selectivity-at-length vectors (bins and mids) ObjFun Fitness deviation metric (for optimization) SPRatsize Cumulative SPR by size class RecProbs, RelRec Recruitment probability and equilibrium recruitment … Other additional diagnostic variables for plotting and analysis 4.2.2 Data Processing Function: processLengthCompData The processLengthCompData() function processes S4 LengthComp objects for use in the GTG dome-shaped LBSPR model, handling both frequency and raw length data with support for grouped or pooled analysis. processLengthCompData(LengthCompObj, byGroup = FALSE, SizeBins = NULL, Lc = 0) 4.2.2.1 Arguments LengthCompObj: S4 LengthComp object containing length data byGroup: Logical, whether to process by group (default FALSE). If TRUE, analyzes each group separately; if FALSE, pools all data together. SizeBins: List with Linc and ToSize elements (optional). If NULL, defaults to 1 cm bins up to reasonable maximum size. Lc: Length at first capture (default 0). For fishery-independent data, must be &gt;= 0. Fish smaller than Lc are removed from analysis. 4.2.2.2 Data processing steps Input Validation: Checks S4 object structure and parameter consistency. Data Extraction: Uses poolLengthComp() to extract data from S4 structure. Binning: For raw length data, creates frequency distributions using hist(). Filtering: Removes fish below Lc for fishery-independent (FI) data. Group Handling: Manages multiple groups based on byGroup argument. 4.2.2.3 Main outputs of the processLengthCompData() function The data processing function processLengthCompData() returns a list containing processed length data: Element Description LenDat Frequency matrix or vector LenMids Length bin midpoints group_names Group identifiers n_groups Number of groups was_pooled Whether data was pooled original_dataType “Frequency” or “Length L_source “FI” or “FD” pooled_data Raw processed data frame binWidth Length bin width used 4.2.3 Optimization function: OptFunDome The OptFunDome() function evaluates how well a proposed set of selectivity and fishing mortality parameters fit observed length-frequency data using a per-recruit simulation and multinomial likelihood. It is used as the objective function in an optimization routine (e.g., within optim()), where the goal is to minimize the negative log-likelihood (NLL) of the predicted length composition relative to observed data. OptFunDome(tryFleetPars, fixedFleetPars, LenDat, lifeHistoryObj, SizeBins = NULL, mod = c(&quot;GTG&quot;, &quot;LBSPR&quot;)) 4.2.3.1 Arguments tryFleetPars: A numeric vector of parameters to estimate (typically log-transformed). For logistic selectivity with estimation, tryFleetPars = c(log(F/M), log(SL50/Linf), log((SL95 - SL50)/Linf)). If logistic selectivity parameters are being estimated, a penalty is added to the objective function if SL50 approaches unrealistic values (i.e., too close to Linf). The penalty is based on a beta distribution. For dome-shaped models (selectivity is fixed)), tryFleetPars = c(log(F/M)). fixedFleetPars: A list of selectivity model parameters to keep fixed during optimization. This must include the selectivityCurve name and any other parameters (e.g., SL1–SL5, SLmesh, etc). LenDat: A vector of observed length-frequency data (number of fish in each length bin). lifeHistoryObj: S4 life history object (same as in GTGDomeLBSPRSim2()) SizeBins: Same as in the GTGDomeLBSPRSim2() function mod: Model type (character); only \"GTG\" is supported (uses growth-type group model). 4.2.3.2 Key steps in the function OptFunDome() The function sets up fleet parameters based on inputs, runs the simulation model, and calculates the negative log-likelihood between observed and predicted length distributions. The main output is a single numeric value that represents the negative log-likelihood (NLL) of the predicted vs. observed length distribution, including a penalty if logistic parameters are being estimated and SL1 approaches unrealistic values (too close to Linf). # Set up fleet parameters Fleet &lt;- NULL Fleet$selectivityCurve &lt;- fixedFleetPars$selectivityCurve # Set selectivity parameters based on curve type if(Fleet$selectivityCurve==&quot;Logistic&quot;){ # ... set logistic parameters } else if(Fleet$selectivityCurve==&quot;Knife&quot;){ # ... set knife-edge parameters } else if(Fleet$selectivityCurve %in% c(&quot;Normal.sca&quot;, &quot;Normal.loc&quot;, &quot;logNorm&quot;, &quot;binorm.sca&quot;,&quot;bilognorm&quot;)){ # ... set dome-shaped parameters } # Set fishing mortality Fleet$FM &lt;- exp(tryFleetPars[1]) # Run the simulation model runMod &lt;- GTGDomeLBSPRSim2(lifeHistoryObj, Fleet, SizeBins) # Calculate negative log-likelihood LenDat &lt;- LenDat + 1E-15 # Add tiny constant to avoid log(0) LenProb &lt;- LenDat/sum(LenDat) # Observed proportions predProb &lt;- runMod$LCatchFished # Predicted proportions predProb &lt;- predProb + 1E-15 # Add tiny constant NLL &lt;- -sum(LenDat * log(predProb/LenProb)) # Negative log-likelihood 4.2.4 Optimization wrapper: DoOptDome This function manages the optimization process to estimate parameters. The function DoOptDome() is the wrapper that runs the full optimization process. It tries to find the best values for fishing mortality (F/M) and, if needed, selectivity parameters like SL1 and SL1 (only for logistic selectivity). To do that, it uses OptFunDome()(i.e., the objective function) to evaluates the likelihood for a given parameter set. This function performs maximum likelihood estimation of fishing mortality and, optionally, selectivity parameters based on observed length-frequency data. It uses a per-recruit simulation (GTGDomeLBSPRSim2()) under the Growth-Type Group (\"GTG\") model framework, and compares observed and predicted length distributions using a multinomial likelihood. 4.2.4.1 Arguments lifeHistoryObj: S4 life history object containing biological parameters fixedFleetPars: A list of fixed fishing/selectivity parameters, including selectivityCurve name, values for SL1–SL5, SLmesh, SLMin, use_aggregated, and fishery_mesh. LenDat: A numeric vector. The observed length-frequency data (e.g., counts per length bin). SizeBins: A list or NULL. A list with Linc (length bin width) and ToSize (maximum length). Defaults are created if not provided. mod: A character. Currently only “GTG” is supported by this code. The function sets up initial parameter values and runs the optimization using either BFGS (when estimating F/M and logistic selectivity) or Brent (when estimating only F/M and dome-shaped selectivity is fixed) methods: 4.2.4.2 Main outputs of the DoOptDome() function The DoOptDome() function returns a list containing: Element Description lbPars Estimated parameters: F/M, SL1 and SL2 (if logistic is estimated), and SPR (derived quantity) lbStdErrs Standard errors of the estimated parameters fixedFleetPars Original (fixed) selectivity settings. PredLen Predicted length-frequency data (expected catch numbers by length bin) NLL Final negative log-likelihood value optimOut Full optim() output object MLE Table of parameter estimates, initial values, and standard errors 4.2.5 User interface functions: DoOptDome.LengthComp Wrapper function for DoOptDome that handles S4 LengthComp objects with support for both grouped and pooled analysis approaches. DoOptDome.LengthComp(lifeHistoryObj, fixedFleetPars, LengthCompObj, SizeBins = NULL, byGroup = FALSE, Lc = 0, mod = c(&quot;GTG&quot;, &quot;LBSPR&quot;)) The DoOptDome.LengthComp function processes S4 LengthComp objects using processLengthCompData(), handles multiple groups either separately (byGroup = TRUE) or pooled (byGroup = FALSE) and returns results appropriate to the analysis approach chosen. DoOptDome.aggregated Convenience wrapper that always pools (aggregates) multiple groups before optimization: DoOptDome.aggregated(lifeHistoryObj, fixedFleetPars, LengthCompObj, SizeBins = NULL, Lc = 0, mod = c(&quot;GTG&quot;, &quot;LBSPR&quot;)) run_grouped_and_pooled Convenience function that runs both grouped and pooled optimization analyses on the same dataset: run_grouped_and_pooled(lifeHistoryObj, fixedFleetPars, LengthCompObj, SizeBins = NULL, Lc = 0, mod = &quot;GTG&quot;) The run_grouped_and_pooled function returns a list with both grouped and pooled results, allowing comparison between approaches. In Chapter 6, the user will find a step-by-step example of applying the GTG Length-Based SPR model, including dome-shaped selectivity (See section 6.2 for details). References Coscino, Connor L., Lyall Bellquist, William J. Harford, and Brice X. Semmens. 2024. “Influence of Life History Characteristics on Data-Limited Stock Status Assertions and Minimum Size Limit Evaluations Using Length-Based Spawning Potential Ratio (LBSPR).” Fisheries Research 276 (August): 107036. https://doi.org/10.1016/j.fishres.2024.107036. Hommik, Kristjan, Ciaran Fitzgerald, Finbarr Kelly, and Samuel Shephard. 2020. “Dome-Shaped Selectivity in LB-SPR: Length-Based Assessment of Data-Limited Inland Fish Stocks Sampled with Gillnets.” Fisheries Research 229: 105574. Hordyk, Adrian R, Kotaro Ono, Jeremy D Prince, and Carl J Walters. 2016b. “A Simple Length-Structured Model Based on Life History Ratios and Incorporating Size-Dependent Selectivity: Application to Spawning Potential Ratios for Data-Poor Stocks.” Canadian Journal of Fisheries and Aquatic Sciences 73 (12): 1787–99. https://doi.org/10.1139/cjfas-2015-0422. Mildenberger, Tobias K., Michael H. Taylor, and Matthias Wolff. 2017. “TropFishR: An r Package for Fisheries Analysis with Length-Frequency Data.” Methods in Ecology and Evolution 8 (11): 1520–27. Millar, Russell B, and Rolf Holst. 1997. “Estimation of Gillnet and Hook Selectivity Using Log-Linear Models.” ICES Journal of Marine Science 54 (3): 471–77. https://doi.org/10.1006/jmsc.1996.0194. "],["utility-functions.html", "5 Utility functions 5.1 poolLengthComp function 5.2 LoptFunc function 5.3 LcFunc function 5.4 fit_gillnet_dome function and gillnet selectivity analysis", " 5 Utility functions 5.1 poolLengthComp function The poolLengthComp function pools length data when multiple columns exist. It sums length frequency data or concatenates length composition data as a single vector. 5.2 LoptFunc function The LoptFunc function calculates optimum harvest length. Derived from Beverton 1992, in a year-class subject to a moderate and constant exponential mortality and whose individuals are growing towards an asymptotic size (as in the Von Bertalanffy growth function), the total biomass of the year-class (i.e. the product of numbers and average weight) reaches a maximum value at some intermediate age (\\(Topt\\)) at a certain weight and length of the individual fish (\\(Wopt\\) and \\(Lopt\\), respectively). \\(Lopt\\) is calculated as: \\[ Lopt = 3 \\frac {L_{\\infty}}{(3 + M/K)} \\] This function is necessary for the PoptFunc function (proportion of fish within optimal sizes in the catch). 5.3 LcFunc function The LcFunc estimates length at full selectivity. This can be done in two methods: 1) using the mode of the length-frequency distribution (LFD); or 2) applying a Kernel smoother to the LFD. For calculating the mode of the LFD, the cumulative distribution of the LFD is computed, and a loess smoother is applied to predict across equally spaced length intervals (1 cm interval); the length at which the cumulative distribution of the predictions increases the most (highest slope) corresponds to the mode of the LFD. Figure 1 shows the cumulative distribution of an example predicted LFD, and the vertical line corresponds to the mode of the distribution. Figure 5.1: Cumulative length-frequency distribution The second method is to apply a Kernel smoother to the LFD and take its maximum value as the length at full selectivity. Figure 2 shows the length at which the Kernel smoother estimates highest density estimates. Figure 5.2: Kernel smoother applied to length-frequency distribution Some results from applying these two functions to example LFDs are presented below. The left-hand plots show an example LFD, and the right-hand side plots show a subset of the example LFD, in order to test the functions with smaller sample sizes and more sparse data. Figure 5.3: Example LFD 2019 Figure 5.4: Example LFD 2018 Figure 5.5: Example LFD 2015 Figure 5.6: Example LFD 2017 These results show that for relatively complete LFDs (higher sample size, left-hand plots), the two methods (mode of LFD and kernel smoother) estimate similar lengths at full selectivity Lc. However, for more sparse length data (right-hand plots), the mode of the LFD might give very different results as for the example LFDs for 2019 and 2017. This is because the highest slope of the cumulative distribution occurred at smaller lengths. The kernel smoother seems to give more stable/reliable estimates of length at full selectivity. 5.4 fit_gillnet_dome function and gillnet selectivity analysis Gillnets retain fish based on size selectivity, where the probability of retention varies with fish length and mesh size. To quantify this selectivity, statistical models (e.g., normal or lognormal curves) are fit to catch data collected across multiple mesh sizes. Each mesh size in a gillnet has its own retention curve, modeled using a probability function. The TropFishR package implements various models using the SELECT method of Russell Millar’s selectivity equations. So the the gillnet selectivity analysis presenetd here is built upon the theoretical framework developed by Millar and Holst (1997) for gillnet selectivity analysis and the modification to that framework made by Mildenberger, Taylor, and Wolff (2017). This document describes the fit_gillnet_dome() function, which fits gillnet selectivity models using the TropFishR package. The function supports multiple dome-shaped selectivity models, and generates diagnostic plots. The fit_gillnet_dome() function is included as a standalone function in the R package fishLengthAssess. The function fits five models to length-frequency data collected from gillnets with different mesh size. The mathematics behind this involves modeling how fish of different sizes are caught by different mesh sizes, accounting for the selectivity characteristics of each mesh. The deviance residuals and goodness-of-fit statistics are used to evaluate model performance. The selectivity models include: norm.loc: Normal (common spread) model norm.sca: Normal (scaled spread) model lognorm: Lognormal model binorm.sca: Bi-normal model bilognorm: Bi-lognormal model 1. Normal Location Model (norm.loc) The normal location model assumes that the selectivity follows a normal distribution with a fixed spread across mesh sizes. Assumes selectivity curves shift with mesh size but maintain the same spread (standard deviation: \\[S(L,m) = \\exp\\left(-\\frac{(L - k \\cdot m)^2}{2\\sigma^2}\\right)\\] Where \\(S(L,m)\\) is the is the selectivity at fish length L and mesh size m, \\(k\\) is a scaling parameter for the modal length (estimated), and \\(\\sigma\\) is the standard deviation (spread of the curve).\\(k\\) and \\(\\sigma\\) are estimated parameters. 2. Normal Scale Model (norm.sca) This model also assumes a normal curve per mesh, but the spread (standard deviation) increases proportionally with mesh size. This model allows selectivity curves to have different widths depending on mesh size. \\[S(L,m) = \\exp\\left(-\\frac{(L - k_1 \\cdot m)^2}{2 \\cdot k_2 \\cdot m^2}\\right)\\] Where \\(k_1\\) determines the modal length, and \\(k_2\\) controls how the standard deviation scales with mesh size. This model allows wider selectivity curves for larger meshes.\\(k_1\\) and \\(k_2\\) are estimated parameters. 3. Lognormal Model (lognorm) The lognormal model assumes that the selectivity follows a lognormal distribution, which often provides a better fit for skewed length-frequency data. This model is commonly used when selectivity increases and decreases asymmetrically around the peak. Single peak, right-skewed. \\[S(L,m) = \\frac{m}{L} \\cdot \\exp\\left(\\mu + \\ln\\left(\\frac{m}{m_1}\\right) - \\frac{\\sigma^2}{2}\\right) \\cdot \\exp\\left(-\\frac{(\\ln(L) - \\mu - \\ln\\left(\\frac{m}{m_1}\\right))^2}{2\\sigma^2}\\right)\\] Where \\(\\mu\\) is the log-scale mean for the reference mesh size, \\(\\sigma\\) is the log-scale standard deviation, and \\(m_1\\) is the reference mesh size (typically the smallest). \\(\\mu\\) and \\(\\sigma\\) are estimated parameters. 4. Bi-normal Scale Model (binorm.sca) The bi-normal scale model combines two normal distributions, often representing different capture processes (e.g., wedging and tangling). Bi-normal distribution models two peaks in selectivity (if distinct Mode1 and Mode2), useful for species with two size classes that are selectively retained. \\[S(L,m) = p \\cdot \\exp\\left(-\\frac{(L - k_1 \\cdot m)^2}{2 \\cdot \\sigma_1 \\cdot m^2}\\right) + (1-p) \\cdot \\exp\\left(-\\frac{(L - k_2 \\cdot m)^2}{2 \\cdot \\sigma_2 \\cdot m^2}\\right)\\] Where \\(p\\) is the proportion of retention assigned to the first peak , \\(k_1, k_2\\) determine the modal lengths for each component, \\(\\sigma_1\\) and \\(\\sigma_2\\) control the standard deviations for each component. \\(p\\) is calculated as: \\[ p = \\frac{\\exp(\\theta)}{1 + \\exp(\\theta)} \\] The estimated parameters in the bi-normal scale model are: \\(k_1\\) and \\(k_2\\), \\(\\sigma_1\\) and \\(\\sigma_2\\), and \\(\\theta\\). \\(\\theta\\) is estimated in logit scale and \\(p\\) is its inverse logit, therefore the final estimate of \\(p\\) is always between 0 and 1 (a valid probability) 5. Bi-lognormal Model (bilognorm) The bi-lognormal model combines two lognormal distributions and captures two peaks (if distinct Mode1 and Mode2) but with lognormal-based selectivity functions, allowing for asymmetric selectivity at both peaks. The selectivity-at-length function for mesh size \\(m\\) is given by: \\[ S(L, m) = p \\cdot \\left( \\frac{m}{L} \\right) \\cdot \\exp\\left( \\mu_1 + \\ln\\left(\\frac{m}{m_1}\\right) - \\frac{\\sigma_1^2}{2} \\right) \\cdot \\exp\\left( -\\frac{ \\left( \\ln(L) - \\mu_1 - \\ln\\left(\\frac{m}{m_1}\\right) \\right)^2 }{2 \\sigma_1^2} \\right) + (1 - p) \\cdot \\left( \\frac{m}{L} \\right) \\cdot \\exp\\left( \\mu_2 + \\ln\\left(\\frac{m}{m_1}\\right) - \\frac{\\sigma_2^2}{2} \\right) \\cdot \\exp\\left( -\\frac{ \\left( \\ln(L) - \\mu_2 - \\ln\\left(\\frac{m}{m_1}\\right) \\right)^2 }{2 \\sigma_2^2} \\right) \\] Where \\(m_1\\) is the reference mesh size (usually the smallest in the set), \\(\\mu_1\\) and \\(\\mu_2\\) are the log-scale location parameters for the two domes, \\(\\sigma_1\\) and \\(\\sigma_2\\) are the log-scale spread parameters for the two domes, and \\(p\\) is the proportion of fish following the first dome (proportion of retention assigned to the first peak), estimated as: \\[ p = \\frac{\\exp(\\theta_5)}{1 + \\exp(\\theta_5)} \\] The estimated parameters in the bi-lognormal model are: \\(\\mu_1\\), \\(\\mu_2\\), \\(\\sigma_1\\) and \\(\\sigma_2\\) in log-scale. \\(\\theta\\) is estimated in logit scale and \\(p\\) is its inverse logit, therefore the final estimate of \\(p\\) is always between 0 and 1 (a valid probability). 5.4.1 Function Overview The fit_gillnet_dome is part of the fishLengthAssess R package and the function contains the following key features: The function allows the user to decide whether to fit all five models (i.e., norm.loc, norm.sca, lognorm, binorm.sca and bilognorm) or to omit the bimodal models (binorm.sca and bilognorm), using the arument run_bimodal=FALSE/TRUE. By default, the bimodal models are omitted (FALSE). Generates selectivity curves and residual plots for each model. Provides statistics to compare model performance. It automatically calculates appropriate starting values for complex selectivity models (e.g., binorm.sca and bilognorm) based on the length distribution of data. Also, it allows the user to use pre-defined starting values when automatic calculation for binorm.sca and bilognorm is suboptimal. Creates diagnostic plots to understand length distributions across mesh sizes. The fit_gillnet_dome() function streamlines the workflow of gillnet selectivity analysis through three main steps: Exploratory analysis and automatic starting value estimation Model fitting for multiple selectivity curves Result visualization and comparative statistics fit_gillnet_dome &lt;- function(input_data, mesh_sizes, run_bimodal=FALSE, manual_x0_list = list(), length_seq = seq(40, 100, 0.1), output_dir = &quot;model_plots&quot;, criterion = &quot;Deviance&quot;, sd_spread = 7, rel.power = NULL, verbose = TRUE) 5.4.2 Function Arguments Arguments Description input_data Data frame with fish lengths in first column and catches for each mesh size in subsequent columns mesh_sizes Vector of mesh sizes in the same order as the columns in input_data run_bimodal Logical (TRUE/FALSE) that controls whether bimodal models (binorm.sca and bilognorm) are fitted. Default: FALSE manual_x0_list Optional list of manually specified starting values for each model type length_seq Sequence of length values for plotting selectivity curves (default: 40-100 cm by 0.1) output_dir Directory where plots will be saved (default: “model_plots”) criterion Criterion for model selection (“Deviance” or “LogLikelihood”) sd_spread Range around detected modes used to calculate starting values for standard deviations (default: 7) rel.power Optional vector of relative fishing powers for each mesh size verbose Whether to print progress messages (default: TRUE) 5.4.3 Data exploration and calculation of starting values The first step of the function performs exploratory analysis to understand the catch distribution across length classes and mesh sizes. It then uses these insights to automatically calculate appropriate starting values for the optimization process. Data visualization Four key plots are generated and saved in the specified output directory: Length distribution by mesh size: Histograms showing catch at each length class for each mesh. Catch distribution across length classes: Line plots comparing catch patterns between mesh sizes. Aggregated length distribution: Bar plot of total catch by length class across all meshes. Detected peaks: Visualization of the identified modes in the length frequency data. Figure 5.7: Exploratory visualization of data. The above plot shows the four exploratory visualizations combined. The first panel (top left) shows the length distribution by mesh size, the second panel (top right) shows the catch distribution across length classes, the third panel (bottom left) shows the aggregated length distribution, and the fourth panel (bottom right) shows the detected peaks (Mode1 in red, Mode2 in blue). Calculation of starting values for binorm.sca and bilognorm An important feature of this function is that it only calculates and provides starting values (x0) for the more complex bimodal models (binorm.sca and bilognorm). For the simpler models (norm.loc, norm.sca, and lognorm), x0 is intentionally set to NULL. This is because: The select_Millar() function internally calls gillnetfit() for these simpler models, which has its own robust mechanism for estimating initial values. Simpler models are less sensitive to starting values and generally converge well without external initialization. Bimodal models have more parameters (including the proportion parameter p) and are more prone to convergence issues without good starting values. The function uses the findpeaks() function from the pracma package to detect modes in the aggregated length distribution: peaks &lt;- findpeaks(total_counts, nups = 2, ndowns = 2, minpeakheight = max(total_counts) * 0.1) nups = 2: Requires at least 2 increasing points before the peak ndowns = 2: Requires at least 2 decreasing points after the peak minpeakheight = max(total_counts) * 0.1: Ignores small peaks (less than 10% of maximum) Starting values calculation The function handles three scenarios: Two or more peaks detected: Mode1 = location of first peak Mode2 = location of second peak Only one peak detected: Mode1 = location of the peak Mode2 = location of highest catch at least 5 cm away from Mode1, or 75th percentile if no suitable secondary peak No clear peaks: Mode1 = 30th percentile of length distribution Mode2 = 70th percentile of length distribution Standard deviations are estimated from the data in windows around each mode: #It keeps all lengths within a window of +/- sd_spread cm around Mode1 and Mode2. #e.g. if Mode1 = 40 and sd_spread = 7, it keeps lengths from 33 to 47 cm subset1 &lt;- plot_data$MidLength[plot_data$MidLength &gt; (Mode1 - sd_spread) &amp; plot_data$MidLength &lt; (Mode1 + sd_spread)] #selects a subset of fish lengths around Mode1 subset2 &lt;- plot_data$MidLength[plot_data$MidLength &gt; (Mode2 - sd_spread) &amp; plot_data$MidLength &lt; (Mode2 + sd_spread)] #selects a subset of fish lengths around Mode2 #Now if If the SD is missing (NA), or too small (less than 3 cm) (too narrow curve). Then it defaults to 3.5 cm as a safe minimum value StdDev1 &lt;- ifelse(is.na(sd(subset1)) || sd(subset1) &lt; 3, 3.5, sd(subset1)) StdDev2 &lt;- ifelse(is.na(sd(subset2)) || sd(subset2) &lt; 3, 4.5, sd(subset2)) For bimodal models, the function also calculates the proportion parameter based on the relative catch in each mode: Catch_Mode1 &lt;- sum(total_counts[plot_data$MidLength &gt; (Mode1 - sd_spread) &amp; plot_data$MidLength &lt; (Mode1 + sd_spread)], na.rm = TRUE) Catch_Mode2 &lt;- sum(total_counts[plot_data$MidLength &gt; (Mode2 - sd_spread) &amp; plot_data$MidLength &lt; (Mode2 + sd_spread)], na.rm = TRUE) P_Mode1 &lt;- min(max(Catch_Mode1 / (Catch_Mode1 + Catch_Mode2), 0.75), 0.95) P_Mode1_logit &lt;- qlogis(P_Mode1) # Convert to logit scale For lognormal-based models, the function converts raw-space parameters to log-space parameters: log_sd_from_raw &lt;- function(mean_val, sd_val) sqrt(log(1 + (sd_val / mean_val)^2)) LogStdDev1 &lt;- min(max(log_sd_from_raw(Mode1, StdDev1), 0.1), 0.4) LogStdDev2 &lt;- min(max(log_sd_from_raw(Mode2, StdDev2), 0.1), 0.4) This transformation uses the mathematical relationship between normal and lognormal distributions: If \\(X \\sim \\text{LogNormal}(\\mu, \\sigma^2)\\), then: - Mode of \\(X = e^{\\mu - \\sigma^2}\\) - Variance of \\(X = (e^{\\sigma^2} - 1) \\cdot e^{2\\mu + \\sigma^2}\\) Solving for \\(\\sigma\\) given the mode and standard deviation: \\(\\sigma = \\sqrt{\\log\\left(1 + \\left(\\frac{\\text{StdDev}}{\\text{Mode}}\\right)^2\\right)}\\) 5.4.4 Model Fitting This step fits multiple selectivity models using the automatically calculated starting values from the previous step or using starting values manually provided if specified. The run_bimodal parameter controls which models are fitted: When run_bimodal = TRUE (default): All five selectivity models are fitted (three unimodal and two bimodal models) When run_bimodal = FALSE: Only the three unimodal models are fitted. Category Models Description Unimodal norm.loc, norm.sca, lognorm Always fitted regardless of run_bimodal setting Bimodal binorm.sca, bilognorm Only fitted when run_bimodal = TRUE The function fit_gillnet_dome prepares the data in the format required by the select_Millar() function: data_list &lt;- list( midLengths = midLengths, meshSizes = mesh_sizes, CatchPerNet_mat = CatchPerNet_mat, rel.power = rel.power ) In the model fitting loop, three or five models are fitted: Normal location (norm.loc) Normal scale (norm.sca) Lognormal (lognorm) Bi-normal scale (binorm.sca) (optional) Bi-lognormal (bilognorm) (optional) Each model in the selected set is fitted sequentially using an error-handling approach: for (model in models) { cat(&quot;\\nFitting model:&quot;, model, &quot;...\\n&quot;) # Determine starting values for each model x0 &lt;- if (model %in% names(full_x0_list)) full_x0_list[[model]] else NULL # Try to fit the model, catch any errors results[[model]] &lt;- tryCatch({ select_Millar(data_list, x0 = x0, rtype = model, rel.power = rel.power, plot = FALSE) }, error = function(e) { cat(&quot;Error fitting model:&quot;, model, &quot;- Skipping.\\n&quot;) return(NULL) }) } results &lt;- results[!sapply(results, is.null)] if (length(results) == 0) { cat(&quot;No models fitted successfully.\\n&quot;) return(NULL) } This loop iterates through each model type (unimodal and bimodal if selected),provides informative output to track progress during model fitting, and determines appropriate starting values: For bimodal models: Uses the automatically detected peaks or manually provided values. For unimodal models: Allows the select_Millar() function to calculate starting values internally. Also, the loops contains a tryCatch() to handling errors: If a model successfully fits: Stores the result in the results list. If a model fails to converge or encounters numerical issues: Captures the error, logs a message, and continues with the next model. This prevents a single problematic model from causing the entire analysis to fail. After the loop completes, the function filters out any failed models before proceeding to model comparison and visualization steps. 5.4.5 Model parameter summary After fitting, a summary table is created with key parameters from each model: summary_table &lt;- data.frame( Model = names(results), LogLikelihood = sapply(results, function(x) x$out[&quot;model.l&quot;, 1]), Deviance = sapply(results, function(x) x$out[&quot;Deviance&quot;, 1]), Mode1 = sapply(results, function(x) x$estimates[1, &quot;par&quot;]), StdDev1 = sapply(results, function(x) x$estimates[2, &quot;par&quot;]), Mode2 = sapply(results, function(x) ifelse(nrow(x$estimates) &gt; 2, x$estimates[3, &quot;par&quot;], NA)), StdDev2 = sapply(results, function(x) ifelse(nrow(x$estimates) &gt; 3, x$estimates[4, &quot;par&quot;], NA)), P_Mode1 = sapply(results, function(x) ifelse(nrow(x$estimates) &gt; 4, x$estimates[5, &quot;par&quot;], NA)) ) Models are sorted by Deviance and Log-likelihood values. Log-Likelihood and Deviance are statistical measures used to evaluate the goodness of fit of different models. The Log-Likelihood value indicates how well a model fits the observed data. Higher values are better (closer to zero, as log-likelihoods are negative). When comparing models with the same number of parameters, the one with the higher log-likelihood provides a better fit. Deviance is derived from the log-likelihood and measures the departure of the model from a perfectly fitting model. It allows for formal statistical tests between nested models. The model with the lowest deviance provides the closest fit to the observed data. Based on the Log-likelihood values, the user can calculate AIC (AIC = -2 × log-likelihood + 2 × k), where k is the number of parameters. The model with the lowest AIC is considered the best balance between goodness-of-fit and parsimony. The following table shows an example of a statistical summary. Table 5.1: Example summary table showing parameter estimates and fit statistics for different models (only unimodal models) Model LogLikelihood Deviance Mode1 StdDev1 lognorm 25014.23 704.28 54.91 4.46 norm.sca 24979.99 772.76 55.28 4.34 norm.loc 24934.91 862.92 54.60 5.15 5.4.6 Result visualization and interpretation The final step creates visual outputs for each successfully fitted model. Selectivity curves For each model, selectivity curves are calculated for all mesh sizes across the specified length range: rmatrix &lt;- outer(plotlens, meshSizes, rtypes_Millar(res$rtype), res$par) rmatrix &lt;- t(t(rmatrix) * res$rel.power) These curves show the relative retention probability for fish of different lengths in each mesh size. Deviance Residuals Deviance residuals help assess model fit by showing where the model predictions differ from observed data: dev_res_df &lt;- data.frame( Length = rep(res$midLengths, times = nmeshes), MeshSize = factor(rep(meshSizes, each = length(res$midLengths))), Residuals = as.vector(res$Dev.resids) ) Bubble plots visualize these residuals, with: - Bubble size proportional to residual magnitude - Blue bubbles for negative residuals (model overprediction) - Red bubbles for positive residuals (model underprediction) Output plots For each model, the function creates and saves: 1. A selectivity curve plot showing relative retention by length for each mesh size 2. A deviance residual bubble plot showing fit quality across lengths and mesh sizes 3. Combined plots with both visualizations together Below are two examples of plots for two different selectivity models: Figure 5.8: Normal location model (norm.loc) selectivity curves and residuals. The top panel shows the selectivity curves for each mesh size, while the bottom panel displays the deviance residuals. Figure 5.9: Lognormal model (lognorm) selectivity curves and residuals. The top panel shows the selectivity curves for each mesh size, while the bottom panel displays the deviance residuals. Each plot shows: Selectivity curve plot (top panel): X-axis: Fish length (cm) Y-axis: Relative retention (scaled to maximum of 1) Lines: Each colored line represents a different mesh size Interpretation: Each curve shows the relative probability of catching fish of different lengths with a specific mesh size The peak of each curve indicates the optimal fish length for that mesh size Curves typically shift to the right as mesh size increases The width of curves indicates the selectivity range Comparing curve shapes across models helps evaluate which provides the most realistic representation Deviance residuals bubble plot (bottom panel) X-axis: Fish length (cm) Y-axis: Mesh size (cm) Bubbles: Size: Indicates the absolute magnitude of the deviance residual Color: Blue for negative residuals (model overestimates), red for positive residuals (model underestimates) Position: Each bubble positioned at specific length × mesh size combination Interpretation: Good-fitting models have smaller bubbles distributed randomly Systematic patterns (e.g., clusters of same-colored bubbles) indicate areas where the model fits poorly Areas with large bubbles indicate specific length-mesh combinations where the model predictions differ substantially from observed data These visualizations are saved for each model in the specified output directory, providing a comprehensive visual assessment of model performance. In Chapter 6, the user will find a a complete example showing how to use the fit_gillnet_dome function with real data (See section 6.1 for details). References Mildenberger, Tobias K., Michael H. Taylor, and Matthias Wolff. 2017. “TropFishR: An r Package for Fisheries Analysis with Length-Frequency Data.” Methods in Ecology and Evolution 8 (11): 1520–27. Millar, Russell B, and Rolf Holst. 1997. “Estimation of Gillnet and Hook Selectivity Using Log-Linear Models.” ICES Journal of Marine Science 54 (3): 471–77. https://doi.org/10.1006/jmsc.1996.0194. "],["examples.html", "6 Examples 6.1 fit_gillnet_dome function 6.2 GTGDomeLBSPRSim2 function", " 6 Examples 6.1 fit_gillnet_dome function This section demonstrates how to use the fit_gillnet_dome() function to estimate selectivity parameters for gillnet fisheries. The function fits multiple selectivity models to experimental gillnet data and provides comprehensive model comparison capabilities. The fit_gillnet_dome() function is designed to: Fit multiple selectivity curves to gillnet experimental data Handle both unimodal and bimodal selectivity patterns Provide automatic model comparison and selection Generate plots and statistical summaries Support both automatic and manual starting value specification The function expects input data with the following structure: Length measurements or frequency data from experimental gillnet fishing Multiple mesh sizes tested simultaneously Data organized in a format compatible with the package’s data processing functions Loading the package and data: # Load the package library(fishLengthAssess) # Load the example gillnet data data(&quot;raw_data_gillnet&quot;) input_data &lt;- raw_data_gillnet input_data ## V1 V2 V3 V4 V5 V6 V7 V8 V9 ## 1 52.5 52 11 1 1 0 0 0 0 ## 2 54.5 102 91 16 4 4 2 0 3 ## 3 56.5 295 232 131 61 17 13 3 1 ## 4 58.5 309 318 362 243 95 26 4 3 ## 5 60.5 118 173 326 342 199 100 10 11 ## 6 62.5 79 87 191 239 202 201 39 15 ## 7 64.5 27 48 111 143 133 185 72 25 ## 8 66.5 14 17 44 51 52 122 74 41 ## 9 68.5 8 6 14 23 25 59 65 76 ## 10 70.5 7 3 8 14 15 16 34 33 ## 11 72.5 0 3 1 2 5 4 6 15 # Define the mesh sizes (cm) used in the experiment mesh_sizes &lt;- c(13.5, 14.0, 14.8, 15.4, 15.9, 16.6, 17.8, 19) 6.1.1 Example 1 6.1.1.1 Fitting unimodal models only In many cases, the user may want to start with simpler unimodal selectivity models before considering more complex bimodal patterns. Setting run_bimodal = FALSE fits only the three basic dome-shaped models (i.e., norm.loc: Normal common spread, norm.sca: Normal scaled spread, and lognorm: Lognormal). # Fit unimodal selectivity models only result_unimodal &lt;- fit_gillnet_dome( input_data = input_data, mesh_sizes = mesh_sizes, run_bimodal = FALSE, output_dir = &quot;model_plots_unimodal&quot;, length_seq = seq(40, 100, 1) ) The function automatically generates plots for each fitted selectivity model and saves them in the specified output directory (e.g., model_plots_unimodal). Figure 6.1: Normal location model (norm.loc) selectivity curves and residuals. The top panel shows the selectivity curves for each mesh size, while the bottom panel displays the deviance residuals. Figure 6.2: Normal scaled spread model (norm.sca) selectivity curves and residuals. The top panel shows the selectivity curves for each mesh size, while the bottom panel displays the deviance residuals. Figure 6.3: Lognormal model (lognorm) selectivity curves and residuals. The top panel shows the selectivity curves for each mesh size, while the bottom panel displays the deviance residuals. The function returns a comprehensive list object with results for each fitted model. The user can access the objects using the $ symbol, as follows: # Available model results names(result_unimodal$results) ## [1] &quot;norm.loc&quot; &quot;norm.sca&quot; &quot;lognorm&quot; # Parameter estimates for Normal Location model print(result_unimodal$results$norm.loc$estimates) ## par s.e. ## Mode(mesh1) 54.600084 0.07305819 ## Std dev.(mesh1) 5.147949 0.06709739 # Parameter estimates for Normal Scale model print(result_unimodal$results$norm.sca$estimates) ## par s.e. ## Mode(mesh1) 55.282277 0.06986564 ## Std dev.(mesh1) 4.343513 0.05268696 # Parameter estimates for Lognormal model print(result_unimodal$results$lognorm$estimates) ## par s.e. ## Mode(mesh1) 54.910902 0.07055028 ## Std dev.(mesh1) 4.457549 0.05828204 Here is an example of how to access the selection ogive (for all mesh sizes) for the norm.loc model. # Selection ogive matrix (selectivity by length and mesh size) print(&quot;Selection Ogive Matrix (first 10 rows):&quot;) ## [1] &quot;Selection Ogive Matrix (first 10 rows):&quot; print(head(result_unimodal$results$norm.loc$selection_ogive_mat, 10)) ## Length 13.5 14 14.8 15.4 ## [1,] 52.5 0.920157978 0.72570316 0.3600821 0.1642659 ## [2,] 54.5 0.999811030 0.91853023 0.5818113 0.3187592 ## [3,] 56.5 0.934163686 0.99971780 0.8083741 0.5318972 ## [4,] 58.5 0.750546437 0.93564461 0.9658110 0.7632066 ## [5,] 60.5 0.518539308 0.75299822 0.9922508 0.9416860 ## [6,] 62.5 0.308060013 0.52110652 0.8765976 0.9991245 ## [7,] 64.5 0.157376001 0.31010488 0.6659300 0.9115545 ## [8,] 66.5 0.069133924 0.15868658 0.4350173 0.7151468 ## [9,] 68.5 0.026115205 0.06982667 0.2443621 0.4824556 ## [10,] 70.5 0.008482917 0.02642117 0.1180350 0.2798781 ## 15.9 16.6 17.8 19 ## [1,] 0.07207511 0.01755223 0.0007710961 1.392758e-05 ## [2,] 0.16292195 0.04912666 0.0031128820 8.109595e-05 ## [3,] 0.31668203 0.11823648 0.0108060347 4.060432e-04 ## [4,] 0.52931815 0.24470078 0.0322566740 1.748215e-03 ## [5,] 0.76078103 0.43548050 0.0827984754 6.472424e-03 ## [6,] 0.94026896 0.66642551 0.1827572547 2.060576e-02 ## [7,] 0.99929577 0.87696859 0.3468777475 5.641049e-02 ## [8,] 0.91324127 0.99235256 0.5661451417 1.327947e-01 ## [9,] 0.71767289 0.96560050 0.7945635876 2.688134e-01 ## [10,] 0.48497246 0.80793886 0.9589126819 4.679190e-01 6.1.1.2 Extracting mesh-specific selectivity The user can extract selectivity curves for specific mesh sizes: # Extract selectivity for mesh size 17.8 cm (Normal Location model) mesh_17_8 &lt;- result_unimodal$selectivity_curves$norm.loc[, c(&quot;Length&quot;, &quot;17.8&quot;)] # Display first 10 rows print(&quot;Selectivity for 17.8 cm mesh (Normal Location model):&quot;) ## [1] &quot;Selectivity for 17.8 cm mesh (Normal Location model):&quot; print(head(mesh_17_8, 20)) ## Length 17.8 ## 1 40 4.112843e-09 ## 2 41 1.349610e-08 ## 3 42 4.264683e-08 ## 4 43 1.297710e-07 ## 5 44 3.802602e-07 ## 6 45 1.072992e-06 ## 7 46 2.915577e-06 ## 8 47 7.628953e-06 ## 9 48 1.922285e-05 ## 10 49 4.664261e-05 ## 11 50 1.089834e-04 ## 12 51 2.452169e-04 ## 13 52 5.313158e-04 ## 14 53 1.108581e-03 ## 15 54 2.227381e-03 ## 16 55 4.309570e-03 ## 17 56 8.029450e-03 ## 18 57 1.440622e-02 ## 19 58 2.489010e-02 ## 20 59 4.141099e-02 # Extract selectivity for mesh size 15.9 cm (Normal common spread model) mesh_15_9 &lt;- result_unimodal$selectivity_curves$norm.sca[, c(&quot;Length&quot;, &quot;15.9&quot;)] # Display first 10 rows print(&quot;Selectivity for 15.9 cm mesh (Normal common spread model):&quot;) ## [1] &quot;Selectivity for 15.9 cm mesh (Normal common spread model):&quot; print(head(mesh_15_9, 20)) ## Length 15.9 ## 1 40 5.864747e-06 ## 2 41 1.501946e-05 ## 3 42 3.702241e-05 ## 4 43 8.783750e-05 ## 5 44 2.005859e-04 ## 6 45 4.408854e-04 ## 7 46 9.327306e-04 ## 8 47 1.899292e-03 ## 9 48 3.722481e-03 ## 10 49 7.022280e-03 ## 11 50 1.275055e-02 ## 12 51 2.228357e-02 ## 13 52 3.748398e-02 ## 14 53 6.068924e-02 ## 15 54 9.457640e-02 ## 16 55 1.418597e-01 ## 17 56 2.048048e-01 ## 18 57 2.845944e-01 ## 19 58 3.806429e-01 ## 20 59 4.900204e-01 6.1.1.3 Calculating aggregated selectivity Use get_composite_curve() to calculate the combined selectivity across all mesh sizes. The following example shows how to get the aggregated selectivity for the Normal Location model (norm.loc). # Calculate aggregated selectivity curve aggregated_selectivity &lt;- get_composite_curve( result_unimodal$results$norm.loc, length_seq = seq(40, 100, 1) ) # Display first 10 values print(&quot;Aggregated selectivity curve (first 10 values):&quot;) ## [1] &quot;Aggregated selectivity curve (first 10 values):&quot; print(head(aggregated_selectivity, 10)) ## Length Selectivity ## 1 40 0.005309309 ## 2 41 0.009262737 ## 3 42 0.015598858 ## 4 43 0.025365112 ## 5 44 0.039841359 ## 6 45 0.060475153 ## 7 46 0.088754824 ## 8 47 0.126021978 ## 9 48 0.173242333 ## 10 49 0.230771713 6.1.1.4 Plotting mesh-specific and aggregated selectivity curves Use plot_mesh_curves() to create plots. The following example shows how to plot selectivity for the Normal Location model (norm.loc). # Plot selectivity curves for each mesh size plot_mesh_curves( result_unimodal, &quot;norm.loc&quot;, length_seq = seq(40, 100, 1), save_plot = TRUE, output_dir = &quot;model_plots_unimodal&quot; ) ## Plot saved to: model_plots_unimodal/Curves_norm.loc.jpeg Figure 6.4: Mesh-specific and composite selectivity curves 6.1.1.5 Comparing unimodal models Use the compare_stats function to generate a comprehensive comparison table. This function allows the user to save a CSV file with the outputs. # Generate comparison table for unimodal models comparison_unimodal &lt;- compare_stats( result_unimodal, include_bimodal = FALSE, save_csv = TRUE, filename = &quot;model_plots_unimodal/unimodal_models_comparison.csv&quot; ) ## Table saved to: model_plots_unimodal/unimodal_models_comparison.csv Table 6.1: Comparison of unimodal selectivity models Model LogLikelihood Deviance Mode1 StdDev1 lognorm 25014.23 704.28 54.91 4.46 norm.sca 24979.99 772.76 55.28 4.34 norm.loc 24934.91 862.92 54.60 5.15 6.1.2 Example 2 6.1.2.1 Fitting all models, including bimodals models For more complex selectivity patterns, include bimodal models (binorm.sca: Bi-normal model and bilognorm: Bi-lognormal) by setting run_bimodal = TRUE. This fits all five available selectivity models using automatic starting values. # Fit all selectivity models result_all_auto &lt;- fit_gillnet_dome( input_data = input_data, mesh_sizes = mesh_sizes, run_bimodal = TRUE, output_dir = &quot;model_plots_all_auto&quot;, length_seq = seq(40, 100, 1) ) ## Warning in sqrt(diag(varpars)): NaNs produced The function automatically generates plots for each fitted selectivity model and saves them in the specified output directory (e.g., model_plots_all_auto). Figure 6.5: Binormal sca (binorm.sca) selectivity curves and residuals. The top panel shows the selectivity curves for each mesh size, while the bottom panel displays the deviance residuals. Figure 6.6: Bilognormal model (bilognorm) selectivity curves and residuals. The top panel shows the selectivity curves for each mesh size, while the bottom panel displays the deviance residuals. The function returns a comprehensive list object with results for each fitted model. The user can access the objects using the $ symbol, as follows: # Available model results names(result_all_auto$results) ## [1] &quot;norm.loc&quot; &quot;norm.sca&quot; &quot;lognorm&quot; &quot;binorm.sca&quot; ## [5] &quot;bilognorm&quot; # Parameter estimates for binorm.sca model print(result_all_auto$results$binorm.sca$estimates) ## par s.e. ## Mode1(mesh1) 54.3225310 0.1316355 ## Std dev.1(mesh1) 3.2269064 0.1337843 ## Mode2(mesh1) 59.6532976 1.1968464 ## Std dev.2(mesh1) 7.2744073 0.7121720 ## P(mode1) 0.7901953 0.0407968 # Parameter estimates for bilognorm model print(result_all_auto$results$bilognorm$estimates) ## par s.e. ## Mode1(mesh1) 54.4858569 0.06528697 ## Std dev.1(mesh1) 3.6471542 0.04494561 ## Mode2(mesh1) 78.3148120 NaN ## Std dev.2(mesh1) 26.0932316 NaN ## P(mode1) 0.8635046 NaN Here is an example of how to access the selection ogive (for all mesh sizes) for the binorm and bilognorm models. # Selection ogive matrix (selectivity by length and mesh size) print(&quot;Selection Ogive Matrix (first 10 rows):&quot;) ## [1] &quot;Selection Ogive Matrix (first 10 rows):&quot; print(head(result_all_auto$results$binorm.sca$selection_ogive_mat, 10)) ## Length 13.5 14 14.8 15.4 ## [1,] 52.5 0.8030715 0.5069798 0.1649934 0.06517818 ## [2,] 54.5 0.9522461 0.8102611 0.3673230 0.15625122 ## [3,] 56.5 0.8202936 0.9521900 0.6570273 0.34183870 ## [4,] 58.5 0.5490164 0.8308804 0.9002506 0.61520946 ## [5,] 60.5 0.3348435 0.5705555 0.9361642 0.86850960 ## [6,] 62.5 0.2261985 0.3538089 0.7550065 0.94975497 ## [7,] 64.5 0.1735095 0.2376248 0.5057878 0.81518812 ## [8,] 66.5 0.1353649 0.1815174 0.3227579 0.57650228 ## [9,] 68.5 0.1002017 0.1435352 0.2267997 0.37316321 ## [10,] 70.5 0.0690314 0.1090331 0.1775600 0.25469460 ## 15.9 16.6 17.8 19 ## [1,] 0.03273717 0.01506024 0.005128329 0.001870887 ## [2,] 0.07387006 0.02925808 0.009044604 0.003350637 ## [3,] 0.17172061 0.06283026 0.015989792 0.005806168 ## [4,] 0.36125853 0.14166006 0.029851447 0.009853976 ## [5,] 0.62930073 0.29908700 0.060872776 0.016856752 ## [6,] 0.87174299 0.54017873 0.130029363 0.030381864 ## [7,] 0.95021819 0.79690933 0.265043607 0.059216942 ## [8,] 0.82480418 0.94445433 0.476509305 0.120588379 ## [9,] 0.59499761 0.90174221 0.723154066 0.237610562 ## [10,] 0.39112094 0.71058796 0.909214143 0.423048376 # Selection ogive matrix (selectivity by length and mesh size) print(&quot;Selection Ogive Matrix (first 10 rows):&quot;) ## [1] &quot;Selection Ogive Matrix (first 10 rows):&quot; print(head(result_all_auto$results$bilognorm$selection_ogive_mat, 10)) ## Length 13.5 14 14.8 15.4 ## [1,] 52.5 0.7906976 0.5115406 0.1625205 0.05905702 ## [2,] 54.5 0.9253369 0.7972326 0.3724503 0.15330716 ## [3,] 56.5 0.8155435 0.9252546 0.6555444 0.34652595 ## [4,] 58.5 0.5687858 0.8246240 0.8782271 0.61595497 ## [5,] 60.5 0.3406743 0.5896322 0.9129551 0.84980985 ## [6,] 62.5 0.2026631 0.3629373 0.7587812 0.92377141 ## [7,] 64.5 0.1430765 0.2177200 0.5260476 0.81115238 ## [8,] 66.5 0.1257811 0.1499820 0.3262018 0.59533941 ## [9,] 68.5 0.1247992 0.1275632 0.2034513 0.38513118 ## [10,] 70.5 0.1281705 0.1243437 0.1464275 0.24024488 ## 15.9 16.6 17.8 19 ## [1,] 0.02911427 0.01589357 0.008682831 0.004965900 ## [2,] 0.06769553 0.02627352 0.011811785 0.006879007 ## [3,] 0.16961054 0.05675317 0.016517877 0.009286981 ## [4,] 0.36629636 0.13794049 0.026750453 0.012384539 ## [5,] 0.62934127 0.30266416 0.054843590 0.017104670 ## [6,] 0.85271088 0.54387485 0.125714700 0.027179493 ## [7,] 0.92411829 0.78508810 0.267416299 0.053236762 ## [8,] 0.81941640 0.91785623 0.481620170 0.115817146 ## [9,] 0.61296306 0.88453746 0.717323772 0.238822026 ## [10,] 0.40528715 0.71934749 0.886234373 0.428552163 6.1.2.2 Extracting mesh-specific selectivity The user can extract selectivity curves for specific mesh sizes: # Extract selectivity for mesh size 14.8 cm (binorm.sca model) mesh_14_8 &lt;- result_all_auto$selectivity_curves$binorm.sca[, c(&quot;Length&quot;, &quot;14.8&quot;)] # Display first 10 rows print(&quot;Selectivity for 14.8 cm mesh (binorm.sca model):&quot;) ## [1] &quot;Selectivity for 14.8 cm mesh (binorm.sca model):&quot; print(head(mesh_14_8, 20)) ## Length 14.8 ## 1 40 0.001316737 ## 2 41 0.001948243 ## 3 42 0.002839152 ## 4 43 0.004078372 ## 5 44 0.005785166 ## 6 45 0.008132920 ## 7 46 0.011405358 ## 8 47 0.016117339 ## 9 48 0.023241646 ## 10 49 0.034567393 ## 11 50 0.053143061 ## 12 51 0.083607577 ## 13 52 0.132023919 ## 14 53 0.204739313 ## 15 54 0.306011971 ## 16 55 0.434788177 ## 17 56 0.581896770 ## 18 57 0.729476686 ## 19 58 0.854002175 ## 20 59 0.932657127 # Extract selectivity for mesh size 16.6 cm (bilognorm model) mesh_16_6 &lt;- result_all_auto$selectivity_curves$bilognorm[, c(&quot;Length&quot;, &quot;16.6&quot;)] # Display first 10 rows print(&quot;Selectivity for 16.6 cm mesh (bilognorm model):&quot;) ## [1] &quot;Selectivity for 16.6 cm mesh (bilognorm model):&quot; print(head(mesh_16_6, 20)) ## Length 16.6 ## 1 40 0.001305255 ## 2 41 0.001688938 ## 3 42 0.002156548 ## 4 43 0.002719410 ## 5 44 0.003389041 ## 6 45 0.004176944 ## 7 46 0.005094418 ## 8 47 0.006152537 ## 9 48 0.007362833 ## 10 49 0.008740446 ## 11 50 0.010314660 ## 12 51 0.012158045 ## 13 52 0.014454991 ## 14 53 0.017638553 ## 15 54 0.022619119 ## 16 55 0.031092032 ## 17 56 0.045834557 ## 18 57 0.070805346 ## 19 58 0.110802443 ## 20 59 0.170498991 6.1.2.3 Calculating aggregated selectivity Use get_composite_curve() to calculate the combined selectivity across all mesh sizes. The following example shows how to get the aggregated selectivity for the Bi-normal model (binorm.sca). # Calculate aggregated selectivity curve aggregated_selectivity &lt;- get_composite_curve( result_all_auto$results$binorm.sca, length_seq = seq(40, 100, 1) ) # Display first 10 values print(&quot;Aggregated selectivity curve (first 10 values):&quot;) ## [1] &quot;Aggregated selectivity curve (first 10 values):&quot; print(head(aggregated_selectivity, 10)) ## Length Selectivity ## 1 40 0.002939409 ## 2 41 0.004303078 ## 3 42 0.006245301 ## 4 43 0.009058137 ## 5 44 0.013276489 ## 6 45 0.019892823 ## 7 46 0.030652673 ## 8 47 0.048325650 ## 9 48 0.076714490 ## 10 49 0.120094088 6.1.2.4 Plotting mesh-specific and aggregated selectivity curves Use plot_mesh_curves() to create plots. The following example shows how to plot selectivity for the Bi-normal model (binorm.sca). # Plot selectivity curves for each mesh size plot_mesh_curves( result_all_auto, &quot;binorm.sca&quot;, length_seq = seq(40, 100, 1), save_plot = TRUE, output_dir = &quot;model_plots_all_auto&quot; ) ## Plot saved to: model_plots_all_auto/Curves_binorm.sca.jpeg Figure 6.7: Mesh-specific and composite selectivity curves 6.1.2.5 Comparing all models Use the compare_stats function to generate a comprehensive comparison table. This function allows the user to save a CSV file with the outputs. # Generate comparison table for unimodal models comparison_all_auto &lt;- compare_stats( result_all_auto, include_bimodal = TRUE, save_csv = TRUE, filename = &quot;model_plots_all_auto/all_models_comparison.csv&quot; ) ## Table saved to: model_plots_all_auto/all_models_comparison.csv Table 6.2: Comparison of all selectivity models Model Log Likelihood Deviance Mode1 cm StdDev1 cm Mode2 cm StdDev2 cm P_Mode1 bilognorm 25093.30 546.13 54.49 3.65 78.31 26.09 0.86 binorm.sca 25091.39 549.96 54.32 3.23 59.65 7.27 0.79 lognorm 25014.23 704.28 54.91 4.46 NA NA NA norm.sca 24979.99 772.76 55.28 4.34 NA NA NA norm.loc 24934.91 862.92 54.60 5.15 NA NA NA 6.1.3 Example 3 6.1.3.1 Fitting all models, including bimodals with manual starting values For better control over the optimization process, especially with bimodal models (binorm.sca``: Bi-normal model andbilognorm`: Bi-lognormal), the user can specify manual starting values: # Define manual starting values for bimodal models manual_starts &lt;- list( binorm.sca = c(55, 4, 65, 4, 3), # Mode1, SD1, Mode2, SD2, logit(P1) bilognorm = c(4, 0.2, 4.2, 0.1, 2) # log(mu1), log(sigma1), log(mu2), log(sigma2), logit(P1) ) # Fit models with manual starting values result_all_manual &lt;- fit_gillnet_dome( input_data = input_data, mesh_sizes = mesh_sizes, manual_x0_list = manual_starts, run_bimodal = TRUE, output_dir = &quot;model_plots_all_manual&quot;, length_seq = seq(40, 100, 1) ) The function automatically generates plots for each fitted selectivity model and saves them in the specified output directory (e.g., model_plots_all_manual). Figure 6.8: Binormal sca (binorm.sca) selectivity curves and residuals. The top panel shows the selectivity curves for each mesh size, while the bottom panel displays the deviance residuals. Figure 6.9: Bilognormal model (bilognorm) selectivity curves and residuals. The top panel shows the selectivity curves for each mesh size, while the bottom panel displays the deviance residuals. The function returns a comprehensive list object with results for each fitted model. The user can access the objects using the $ symbol, as follows: # Available model results names(result_all_manual$results) ## [1] &quot;norm.loc&quot; &quot;norm.sca&quot; &quot;lognorm&quot; &quot;binorm.sca&quot; ## [5] &quot;bilognorm&quot; # Parameter estimates for binorm.sca model print(result_all_manual$results$binorm.sca$estimates) ## par s.e. ## Mode1(mesh1) 54.3235714 0.13798557 ## Std dev.1(mesh1) 3.2780203 0.13355660 ## Mode2(mesh1) 59.8074724 1.32748734 ## Std dev.2(mesh1) 7.3256977 0.79623248 ## P(mode1) 0.7923451 0.04245935 # Parameter estimates for bilognorm model print(result_all_manual$results$bilognorm$estimates) ## par s.e. ## Mode1(mesh1) 54.4756989 0.08838970 ## Std dev.1(mesh1) 3.6144047 0.10520153 ## Mode2(mesh1) 65.4477215 4.85438585 ## Std dev.2(mesh1) 15.2583192 4.46442782 ## P(mode1) 0.8885962 0.01697675 Here is an example of how to access the selection ogive (for all mesh sizes) for the binorm and bilognorm models (models fitted using manual starting values). # Selection ogive matrix (selectivity by length and mesh size) print(&quot;Selection Ogive Matrix (first 10 rows):&quot;) ## [1] &quot;Selection Ogive Matrix (first 10 rows):&quot; print(head(result_all_manual$results$binorm.sca$selection_ogive_mat, 10)) ## Length 13.5 14 14.8 15.4 ## [1,] 52.5 0.80501771 0.5139128 0.1706424 0.06745457 ## [2,] 54.5 0.95091906 0.8120463 0.3750070 0.16168890 ## [3,] 56.5 0.82314310 0.9508570 0.6618921 0.34950372 ## [4,] 58.5 0.55628162 0.8334593 0.8999424 0.62075937 ## [5,] 60.5 0.34100575 0.5776670 0.9355857 0.86894954 ## [6,] 62.5 0.22940270 0.3603052 0.7593693 0.94862310 ## [7,] 64.5 0.17553847 0.2411976 0.5132374 0.81816555 ## [8,] 66.5 0.13760668 0.1836167 0.3286779 0.58356448 ## [9,] 68.5 0.10277695 0.1456849 0.2300231 0.37994454 ## [10,] 70.5 0.07157392 0.1115508 0.1796182 0.25881336 ## 15.9 16.6 17.8 19 ## [1,] 0.03342761 0.01503676 0.005061236 0.001854765 ## [2,] 0.07653556 0.02978049 0.008948484 0.003310710 ## [3,] 0.17752318 0.06499816 0.015989289 0.005730395 ## [4,] 0.36894296 0.14671383 0.030402075 0.009759185 ## [5,] 0.63462837 0.30658976 0.062949174 0.016880027 ## [6,] 0.87210677 0.54675243 0.134747375 0.030957894 ## [7,] 0.94906242 0.79899252 0.272286992 0.061215201 ## [8,] 0.82753924 0.94317675 0.483716104 0.125013155 ## [9,] 0.60188804 0.90231195 0.726789403 0.244550345 ## [10,] 0.39811903 0.71582689 0.908696457 0.430594111 # Selection ogive matrix (selectivity by length and mesh size) print(&quot;Selection Ogive Matrix (first 10 rows):&quot;) ## [1] &quot;Selection Ogive Matrix (first 10 rows):&quot; print(head(result_all_manual$results$bilognorm$selection_ogive_mat, 10)) ## Length 13.5 14 14.8 15.4 ## [1,] 52.5 0.8250696 0.5327307 0.1696513 0.06261988 ## [2,] 54.5 0.9661127 0.8319239 0.3876150 0.16011951 ## [3,] 56.5 0.8503227 0.9660310 0.6833954 0.36062239 ## [4,] 58.5 0.5918750 0.8598659 0.9168711 0.64193936 ## [5,] 60.5 0.3540786 0.6136509 0.9529089 0.88707225 ## [6,] 62.5 0.2095821 0.3772698 0.7907281 0.96438700 ## [7,] 64.5 0.1442487 0.2255093 0.5472609 0.84570893 ## [8,] 66.5 0.1201446 0.1522825 0.3389986 0.61961430 ## [9,] 68.5 0.1110202 0.1235701 0.2104182 0.40038574 ## [10,] 70.5 0.1053521 0.1127812 0.1481840 0.24920348 ## 15.9 16.6 17.8 19 ## [1,] 0.03118562 0.01642821 0.007717878 0.003650354 ## [2,] 0.07158238 0.02811931 0.011482361 0.005666578 ## [3,] 0.17698821 0.06022595 0.017166514 0.008429273 ## [4,] 0.38120570 0.14422724 0.028636484 0.012183348 ## [5,] 0.65595410 0.31499839 0.058240207 0.017855118 ## [6,] 0.89011485 0.56652779 0.131587321 0.029100847 ## [7,] 0.96475853 0.81918618 0.278378292 0.056568090 ## [8,] 0.85439259 0.95835970 0.501475932 0.121356278 ## [9,] 0.63803374 0.92291611 0.748131310 0.248701381 ## [10,] 0.42137967 0.74938107 0.925264231 0.446090728 6.1.3.2 Extracting Mesh-Specific Selectivity The user can extract selectivity curves for specific mesh sizes: # Extract selectivity for mesh size 14.8 cm (binorm.sca model) mesh_14_8 &lt;- result_all_manual$selectivity_curves$binorm.sca[, c(&quot;Length&quot;, &quot;14.8&quot;)] # Display first 10 rows print(&quot;Selectivity for 14.8cm mesh (binorm.sca model):&quot;) ## [1] &quot;Selectivity for 14.8cm mesh (binorm.sca model):&quot; print(head(mesh_14_8, 10)) ## Length 14.8 ## 1 40 0.001308562 ## 2 41 0.001930941 ## 3 42 0.002807605 ## 4 43 0.004026767 ## 5 44 0.005709644 ## 6 45 0.008038522 ## 7 46 0.011320713 ## 8 47 0.016120196 ## 9 48 0.023494027 ## 10 49 0.035348129 # Extract selectivity for mesh size 16.6 cm (bilognorm model) mesh_16_6 &lt;- result_all_manual$selectivity_curves$bilognorm[, c(&quot;Length&quot;, &quot;16.6&quot;)] # Display first 10 rows print(&quot;Selectivity for 16.6cm mesh (bilognorm model):&quot;) ## [1] &quot;Selectivity for 16.6cm mesh (bilognorm model):&quot; print(head(mesh_16_6, 10)) ## Length 16.6 ## 1 40 0.0005640103 ## 2 41 0.0008139377 ## 3 42 0.0011495654 ## 4 43 0.0015910494 ## 5 44 0.0021605543 ## 6 45 0.0028817911 ## 7 46 0.0037794337 ## 8 47 0.0048785497 ## 9 48 0.0062045436 ## 10 49 0.0077852677 6.1.3.3 Calculating aggregated selectivity The user can use get_composite_curve() to calculate the combined selectivity across all mesh sizes. The following example shows how to get the aggregated selectivity for the Bi-lognormal model (bilognorm). # Calculate aggregated selectivity curve aggregated_selectivity &lt;- get_composite_curve( result_all_manual$results$bilognorm, length_seq = seq(40, 100, 1) ) # Display first 10 values print(&quot;Aggregated selectivity curve (first 10 values):&quot;) ## [1] &quot;Aggregated selectivity curve (first 10 values):&quot; print(head(aggregated_selectivity, 10)) ## Length Selectivity ## 1 40 0.005010173 ## 2 41 0.006677579 ## 3 42 0.008788167 ## 4 43 0.011541483 ## 5 44 0.015408243 ## 6 45 0.021423536 ## 7 46 0.031563602 ## 8 47 0.049008538 ## 9 48 0.077958682 ## 10 49 0.122730987 6.1.3.4 Plotting mesh-specific and aggregated selectivity curves Use plot_mesh_curves() to create plots. The following example shows how to plot selectivity for the Bi-normal model (binorm.sca). # Plot selectivity curves for each mesh size plot_mesh_curves( result_all_manual, &quot;binorm.sca&quot;, length_seq = seq(40, 100, 1), save_plot = TRUE, output_dir = &quot;model_plots_all_manual&quot; ) ## Plot saved to: model_plots_all_manual/Curves_binorm.sca.jpeg Figure 6.10: Mesh-specific selectivity curves 6.1.3.5 Comparing all models As before, the user can use compare_stats() function to generate a comprehensive comparison table. This function allows the user to save a CSV file with the outputs. # Generate comparison table for unimodal models comparison_all_manual &lt;- compare_stats( result_all_manual, include_bimodal = TRUE, save_csv = TRUE, filename = &quot;model_plots_all_manual/all_models_comparison.csv&quot; ) ## Table saved to: model_plots_all_manual/all_models_comparison.csv Table 6.3: Comparison of all selectivity models Model Log Likelihood Deviance Mode1 cm StdDev1 cm Mode2 cm StdDev2 cm P_Mode1 bilognorm 25093.60 545.54 54.48 3.61 65.45 15.26 0.89 binorm.sca 25091.12 550.49 54.32 3.28 59.81 7.33 0.79 lognorm 25014.23 704.28 54.91 4.46 NA NA NA norm.sca 24979.99 772.76 55.28 4.34 NA NA NA norm.loc 24934.91 862.92 54.60 5.15 NA NA NA 6.2 GTGDomeLBSPRSim2 function 6.2.1 Practical Application of GTG Length-Based SPR model including dome-shaped selectivity: Step-by-Step Example This example demonstrates how to perform a dome-shaped length-based spawning potential ratio (LBSPR) simulation using the Growth-Type Group model with support for dome-shaped gillnet selectivity curves. This example demonstrates how to set up the lengthComp object, create the LifeHistoryObj, define selectivity models, run simulations, estimate parameters from length data, and visualize results. The example code presented here was adapted from the original implementation (https://github.com/KHommik/DomeShaped_GTG_LBSPR) by Hommik et al. (2020), with modifications to accommodate additional selectivity models and updated simulation routines. 6.2.1.1 Step 1. Setting Up the Environment Load the required packages and data: library(fishLengthAssess) library(fishSimGTG) # Load the example gillnet data data(&quot;raw_data_gillnet&quot;) input_data &lt;- raw_data_gillnet input_data ## V1 V2 V3 V4 V5 V6 V7 V8 V9 ## 1 52.5 52 11 1 1 0 0 0 0 ## 2 54.5 102 91 16 4 4 2 0 3 ## 3 56.5 295 232 131 61 17 13 3 1 ## 4 58.5 309 318 362 243 95 26 4 3 ## 5 60.5 118 173 326 342 199 100 10 11 ## 6 62.5 79 87 191 239 202 201 39 15 ## 7 64.5 27 48 111 143 133 185 72 25 ## 8 66.5 14 17 44 51 52 122 74 41 ## 9 68.5 8 6 14 23 25 59 65 76 ## 10 70.5 7 3 8 14 15 16 34 33 ## 11 72.5 0 3 1 2 5 4 6 15 # Define the mesh sizes (cm) used in the experiment mesh_sizes &lt;- c(13.5, 14.0, 14.8, 15.4, 15.9, 16.6, 17.8, 19) # Load the example of catch at length data data(&quot;gtg_catch_frequency&quot;) # it contains frequency data(&quot;gtg_catch_lengths&quot;) # it contains lengths # In this example we will use frequency data sampleCatch &lt;- gtg_catch_frequency #sampleCatch &lt;- gtg_catch_lengths # Uncomment this line if you want to use length data 6.2.1.2 Step 2. Create the lengthComp object Set up the characteristics of the data: lengthComp &lt;- new(&quot;LengthComp&quot;) lengthComp@dataType &lt;- &quot;Frequency&quot; # &quot;Length&quot; or &quot;Frequency&quot; - the users need to choose one. lengthComp@L_source &lt;- &quot;FD&quot; lengthComp@dt &lt;- sampleCatch lengthComp@L_units &lt;- &quot;cm&quot; lengthComp@L_type &lt;- &quot;TL&quot; lengthComp@header &lt;-TRUE lengthComp@observationGroup &lt;- &quot;Catch&quot; #for Frequency or &quot;Year&quot; for Length (choose the right one) Note: If sampleCatch &lt;- gtg_catch_frequency then set lengthComp@dataType &lt;- \"Frequency\" and lengthComp@observationGroup &lt;- \"Catch\" If sampleCatch &lt;- gtg_catch_lengths then set lengthComp@dataType &lt;- \"Length\" and lengthComp@observationGroup &lt;- \"Year\" 6.2.1.3 Step 3. Create the LifeHistoryObj object Set up the stock’s biological parameters using direct slot assignment: LifeHistoryObj &lt;- new(&quot;LifeHistory&quot;) LifeHistoryObj@title &lt;- &quot;Fish Stock&quot; LifeHistoryObj@speciesName &lt;- &quot;Generic Fish&quot; LifeHistoryObj@shortDescription &lt;- &quot;Stock for dome shaped LBSPR Analysis&quot; LifeHistoryObj@L_type &lt;- &quot;TL&quot; LifeHistoryObj@L_units &lt;- &quot;cm&quot; LifeHistoryObj@Walpha_units &lt;- &quot;g&quot; LifeHistoryObj@Linf &lt;- 120 LifeHistoryObj@K &lt;- 0.2 LifeHistoryObj@t0 &lt;- 0 LifeHistoryObj@L50 &lt;- 60 LifeHistoryObj@L95delta &lt;- 2 # L95 - L50 = 62 - 60 LifeHistoryObj@M &lt;- 0.3 # Calculated from MK * K = 1.5 * 0.2 LifeHistoryObj@MK &lt;- 1.5 LifeHistoryObj@LW_A &lt;- 0.01 LifeHistoryObj@LW_B &lt;- 3 LifeHistoryObj@Steep &lt;- 0.7 LifeHistoryObj@R0 &lt;- 1E6 LifeHistoryObj@Tmax &lt;- -log(0.01) / LifeHistoryObj@M LifeHistoryObj@recSD &lt;- 0.6 LifeHistoryObj@recRho &lt;- 0 LifeHistoryObj@isHermaph &lt;- FALSE LifeHistoryObj@H50 &lt;- 0 LifeHistoryObj@H95delta &lt;- 0 # GTG-specific parameters as attributes attr(LifeHistoryObj, &quot;CVLinf&quot;) &lt;- 0.1 attr(LifeHistoryObj, &quot;MaxSD&quot;) &lt;- 2 attr(LifeHistoryObj, &quot;NGTG&quot;) &lt;- 13 attr(LifeHistoryObj, &quot;Mpow&quot;) &lt;- 0 attr(LifeHistoryObj, &quot;FecB&quot;) &lt;- 3 6.2.1.4 Step 4. Setting up size bins Define length bins for the analysis: lengthBinWidth &lt;- 1 SizeBins &lt;- list() SizeBins$Linc &lt;- lengthBinWidth SDLinf &lt;- attr(LifeHistoryObj, &quot;CVLinf&quot;) * LifeHistoryObj@Linf SizeBins$ToSize &lt;- LifeHistoryObj@Linf + SDLinf * attr(LifeHistoryObj, &quot;MaxSD&quot;) LenBins &lt;- seq(from = 0, to = SizeBins$ToSize, by = SizeBins$Linc) LenMids &lt;- seq(from=0.5*SizeBins$Linc, by=SizeBins$Linc,length.out=(length(LenBins)-1)) lengthFish &lt;- seq(from=0.5*SizeBins$Linc, by=SizeBins$Linc,length.out=length(LenBins) - 1) 6.2.1.5 Step 5. Fitting selectivity curves to gillnet data If gillnet selectivity data are available, the fit_gillnet_dome() function can be used to fit a variety of dome-shaped selectivity models to these data. This function is described in detail in 5.4 and allows users to estimate selectivity curves such as normal, lognormal, and bimodal types. In this section, we apply fit_gillnet_dome() to example gillnet data from Millar and Holst (1997) to obtain fitted selectivity curves that can later be compared with those used in simulation. selfit &lt;- fit_gillnet_dome( input_data = input_data, mesh_sizes = mesh_sizes, output_dir = &quot;fig_dome_lbspr&quot;, length_seq = lengthFish, run_bimodal = FALSE # Set to TRUE to include bimodal models # Note: If in fit_gillnet_dome(), run_bimodal=FALSE: &quot;Normal.loc&quot;, &quot;Normal.sca&quot;, &quot;logNorm&quot; # Note: If in fit_gillnet_dome(), run_bimodal=TRUE: All the above plus &quot;binorm.sca&quot;, &quot;bilognorm ) We are plotting here the norm.sca: Normal scaled spread selectivity model Figure 6.11: Gillnet selectivity estimation and model residuals. 6.2.1.6 Step 6. Defining fleet parameters The next step is to define a list named FleetPars that contains the fishery parameters. FleetPars &lt;- NULL FleetPars$FM &lt;- 1 The user can choose from several selectivity options by commenting and uncommneting a block of code. This example is based on FleetPars$selectivityCurve &lt;- \"Normal.sca\" Choose one of the selectivity curve options below by uncommenting the desired block and commenting out all others. If FleetPars$use_aggregated &lt;- FALSE, then FleetPars$fishery_mesh must be assigned a specific mesh size. For example, to retrieve the \"Normal.sca\" selectivity curve for mesh size 14, use FleetPars$fishery_mesh &lt;- 14. # Option 0. logistic # FleetPars$selectivityCurve &lt;- &quot;Logistic&quot; # FleetPars$SL1 = 50 # FleetPars$SL2 = 80 # Option 1. Normal Location (normal with fixed spread) # FleetPars$selectivityCurve &lt;- &quot;Normal.loc&quot; # FleetPars$SL1 &lt;- selfit$results$norm.loc$par[1] # FleetPars$SL2 &lt;- selfit$results$norm.loc$par[2] # FleetPars$SLmesh &lt;- mesh_sizes # FleetPars$SLMin &lt;- NA # FleetPars$use_aggregated &lt;- TRUE #FleetPars$fishery_mesh &lt;- 14 # Option 2. Normal.sca (normal with proportional spread) FleetPars$selectivityCurve &lt;- &quot;Normal.sca&quot; FleetPars$SL1 &lt;- selfit$results$norm.sca$par[1] FleetPars$SL2 &lt;- selfit$results$norm.sca$par[2]^2 FleetPars$SLmesh &lt;- mesh_sizes FleetPars$SLMin &lt;- NA FleetPars$use_aggregated &lt;- TRUE # FleetPars$fishery_mesh &lt;- 14 # Option 3. logNorm # FleetPars$selectivityCurve &lt;- &quot;logNorm&quot; # FleetPars$SL1 &lt;- selfit$results$lognorm$par[1] # FleetPars$SL2 &lt;- selfit$results$lognorm$par[2] # FleetPars$SLmesh &lt;- mesh_sizes # FleetPars$SLMin &lt;- NA # FleetPars$use_aggregated &lt;- FALSE # FleetPars$fishery_mesh &lt;- 14 # Option 4. binorm.sca # FleetPars$selectivityCurve &lt;- &quot;binorm.sca&quot; # FleetPars$SL1 &lt;- selfit$results$binorm.sca$par[1] # FleetPars$SL2 &lt;- selfit$results$binorm.sca$par[2] # FleetPars$SL3 &lt;- selfit$results$binorm.sca$par[3] # FleetPars$SL4 &lt;- selfit$results$binorm.sca$par[4] # FleetPars$SL5 &lt;- selfit$results$binorm.sca$par[5] # FleetPars$SLmesh &lt;- mesh_sizes # FleetPars$SLMin &lt;- NA # FleetPars$use_aggregated &lt;- FALSE # FleetPars$fishery_mesh &lt;- 14 # Option 5. bilognorm # FleetPars$selectivityCurve &lt;- &quot;bilognorm&quot; # FleetPars$SL1 &lt;- selfit$results$bilognorm$par[1] # FleetPars$SL2 &lt;- selfit$results$bilognorm$par[2] # FleetPars$SL3 &lt;- selfit$results$bilognorm$par[3] # FleetPars$SL4 &lt;- selfit$results$bilognorm$par[4] # FleetPars$SL5 &lt;- selfit$results$bilognorm$par[5] # FleetPars$SLmesh &lt;- mesh_sizes # FleetPars$SLMin &lt;- NA # FleetPars$use_aggregated &lt;- FALSE # FleetPars$fishery_mesh &lt;- 14 The following lines of code set the gear selectivity variables for use in other parts of the script, create a shorter-named local variable meshSize that references the same mesh sizes as FleetPars$SLmesh (to make subsequent code more concise), and display the selected curve used to run the GTG dome-shaped LBSPR model. gearSelectivity &lt;- FleetPars$selectivityCurve if(!is.null(FleetPars$SLmesh)) meshSize &lt;- FleetPars$SLmesh cat(&quot;Selected selectivity curve:&quot;, gearSelectivity, &quot;\\n&quot;) ## Selected selectivity curve: Normal.sca 6.2.1.7 Step 7. Running the simulation for checking purposes Once the user has defined their FleetPars, they can call the GTGDomeLBSPRSim2() function to run the simulation. After running the simulation, the user can obtain the selectivity curves and check whether GTGDomeLBSPRSim2() reproduces the same curve estimated by the fit_gillnet_dome() function. sim_output &lt;- GTGDomeLBSPRSim2(LifeHistoryObj, FleetPars, SizeBins) gearSelLen &lt;- sim_output$SelLen 6.2.1.8 Step 8. Visualizing selectivity curves After fitting dome-shaped selectivity models using fit_gillnet_dome(), you can compare the resulting fitted curves with those reconstructed internally by the GTGDomeLBSPRSim2() simulation. This step helps verify that the simulation is correctly using the estimated selectivity parameters. The comparison can be done in two modes: Aggregated selectivity (FleetPars$use_aggregated = TRUE): This plots the composite selectivity curve (aggregated across mesh sizes) used in the simulation alongside the one estimated by fit_gillnet_dome(). Mesh-specific selectivity (FleetPars$use_aggregated = FALSE). This plots the selectivity curve corresponding to a specific mesh size (as specified in FleetPars$fishery_mesh) and compares it to the fitted mesh-specific curve. Aggregated selectivity curve are obtaining by using the function get_composite_curve() included in the R code presented in chapter 5, section 5.4. Note: These comparisons apply only to dome-shaped selectivity models (e.g., normal, log-normal, bimodal). A separate comparison routine for logistic selectivity is provided at the end of this document # Map model names model_lookup &lt;- c( &quot;Normal.loc&quot; = &quot;norm.loc&quot;, &quot;Normal.sca&quot; = &quot;norm.sca&quot;, &quot;logNorm&quot; = &quot;lognorm&quot;, &quot;binorm.sca&quot; = &quot;binorm.sca&quot;, &quot;bilognorm&quot; = &quot;bilognorm&quot; ) # Get the selectivity curve name for plotting sel_model_name &lt;- model_lookup[FleetPars$selectivityCurve] # Get selectivity curve from simulation output for plotting gearSelLen &lt;- sim_output$SelLen[1:length(lengthFish)] # For aggregated selectivity if (isTRUE(FleetPars$use_aggregated)) { # Get fitted model from selfit fitted_model &lt;- selfit$results[[sel_model_name]] # Get the aggregated selectivity curve fitted_sel_df &lt;- get_composite_curve(fitted_model, length_seq = lengthFish) colnames(fitted_sel_df) &lt;- c(&quot;Length&quot;, &quot;FittedSelectivity&quot;) # Prepare data for comparison internal_sel_df &lt;- data.frame(Length = lengthFish, gearSelLen) comparison_df &lt;- merge(internal_sel_df, fitted_sel_df, by = &quot;Length&quot;) # Plot comparison p1=ggplot(comparison_df, aes(x = Length)) + geom_line(aes(y = gearSelLen, color = &quot;GTG gearSelLen&quot;), size = 1.2) + geom_line(aes(y = FittedSelectivity, color = &quot;Gillnet fitted&quot;), linetype = &quot;dashed&quot;, size = 1.2) + geom_vline(aes(xintercept = LifeHistoryObj@L50, color = &quot;L50 = mat 50%&quot;), linetype = &quot;dotted&quot;, size = 1) + scale_color_manual(values = c(&quot;GTG gearSelLen&quot; = &quot;blue&quot;, &quot;Gillnet fitted&quot; = &quot;red&quot;,&quot;L50 = mat 50%&quot; = &quot;grey&quot;)) + labs(title = paste(&quot;Selectivity Comparison -&quot;, FleetPars$selectivityCurve), x = &quot;Length (cm)&quot;, y = &quot;Selectivity&quot;, color = &quot;Source&quot;) + theme_bw() # specific mesh selectivity exploration } else if (!is.null(FleetPars$fishery_mesh)) { mesh_use &lt;- as.character(FleetPars$fishery_mesh) fitted_sel_vec &lt;- selfit$selectivity_curves[[sel_model_name]][[mesh_use]] comparison_df &lt;- data.frame( Length = lengthFish, internal_sel = gearSelLen, fitted_sel = fitted_sel_vec ) p1= ggplot(comparison_df, aes(x = Length)) + geom_line(aes(y = internal_sel, color = &quot;gearSelLen&quot;), size = 1.2) + geom_line(aes(y = fitted_sel, color = &quot;gillnet fitted&quot;), linetype = &quot;dashed&quot;, size = 1.2) + geom_vline(aes(xintercept = LifeHistoryObj@L50, color = &quot;L50 = mat 50%&quot;), linetype = &quot;dotted&quot;, size = 1) + scale_color_manual(values = c(&quot;gearSelLen&quot; = &quot;blue&quot;, &quot;gillnet fitted&quot; = &quot;red&quot;,&quot;L50 = mat 50%&quot; = &quot;grey&quot;)) + labs( title = paste(&quot;Selectivity Comparison (Mesh&quot;, mesh_use, &quot;mm)&quot;), x = &quot;Length (cm)&quot;, y = &quot;Selectivity&quot;, color = &quot;Source&quot; ) + theme_bw() } else { stop(&quot;Selectivity comparison cannot proceed: either use_aggregated must be TRUE or a mesh size must be provided.&quot;) } print (p1) Figure 6.12: Comparison of the fitted curves with those reconstructed internally by the GTGDomeLBSPRSim2 simulation 6.2.1.9 Step 9. Parameter estimation To estimate parameters from length-frequency data use the function run_grouped_and_pooled. results_all &lt;- run_grouped_and_pooled(LifeHistoryObj, FleetPars, lengthComp, SizeBins, Lc = 0) ## Running optimization by group... ## Processing group: Catch_1 ## Processing group: Catch_2 ## Processing group: Catch_3 ## Processing group: Catch_4 ## Processing group: Catch_5 ## Running pooled optimization... # The user can explore the outputs using $ symbol print(results_all$pooled$lbPars) ## F/M SPR ## 0.9145970 0.5915958 print(results_all$pooled$lbStdErrs) ## F/M ## 0.1145878 print(results_all$pooled$NLL) ## [1] 31.61274 print(results_all$pooled$optimOut$convergence) ## [1] 0 if (!is.null(results_all$grouped$group_results)) { lapply(results_all$grouped$group_results, function(x) print(x$lbPars)) } ## F/M SPR ## 0.8275995 0.6212810 ## F/M SPR ## 0.7078621 0.6648111 ## F/M SPR ## 0.9810176 0.5699689 ## F/M SPR ## 1.1417950 0.5210937 ## F/M SPR ## 0.9197611 0.5898828 ## $Catch_1 ## F/M SPR ## 0.8275995 0.6212810 ## ## $Catch_2 ## F/M SPR ## 0.7078621 0.6648111 ## ## $Catch_3 ## F/M SPR ## 0.9810176 0.5699689 ## ## $Catch_4 ## F/M SPR ## 1.1417950 0.5210937 ## ## $Catch_5 ## F/M SPR ## 0.9197611 0.5898828 6.2.1.10 Step 10. Visualization The visualization section provides a diagnostic plot to evaluate model performance. The plots help assess how well the estimated parameters reproduce the observed length composition data and provide insights into the biological realism of the fitted model. The visualization section automatically adapts to the input data type specified in the lengthComp object. For frequency data (dataType = \"Frequency\"), observed counts are calculated by summing across catch columns, using the length bins directly from the data structure. For raw length measurements (dataType = \"Length\"), the individual length observations are converted into frequency distributions using the predefined length bins (LenBins) and midpoints (LenMids). # Determine data type and prepare accordingly data_type &lt;- lengthComp@dataType if (data_type == &quot;Frequency&quot;) { cat(&quot;Using Frequency data plotting approach\\n&quot;) # For frequency data - data already has proper structure observed_data &lt;- rowSums(sampleCatch[, -1]) # Sum across catch columns (excluding Length column) length_vector &lt;- sampleCatch$Length # Use the length column } else if (data_type == &quot;Length&quot;) { cat(&quot;Using Length data plotting approach\\n&quot;) # For raw length data - convert to frequency bins observed_data &lt;- hist(unlist(sampleCatch), breaks = LenBins, plot = FALSE)$counts length_vector &lt;- LenMids # Use bin midpoints } else { stop(&quot;Unknown data type: &quot;, data_type) } ## Using Frequency data plotting approach # Get predicted data from optimization results predicted_data &lt;- results_all$pooled$PredLen # Create comparison dataframe compare_df &lt;- data.frame( Length = length_vector, Observed = observed_data, Predicted = predicted_data ) The following plot compares the observed length composition against the model’s predicted values during the parameter estimation process. This plot is helpful to demonstrate how well the optimizer was able to fit the model to the data during the likelihood maximization. # Basic comparison of observed vs predicted during optimization p_basic &lt;- ggplot(compare_df, aes(x = Length)) + geom_bar(aes(y = Observed), stat = &quot;identity&quot;, fill = &quot;grey&quot;, alpha = 0.7) + geom_line(aes(y = Predicted), color = &quot;red&quot;, size = 1.2) + theme_bw() + labs(title = paste(&quot;Observed vs Predicted Length Composition (&quot;, data_type, &quot;data)&quot;), x = &quot;Length (cm)&quot;, y = &quot;Counts&quot;) print(p_basic) Figure 6.13: Observed vs Predicted Length Composition 6.2.1.11 Step 11. Model re-simulation setup After parameter estimation, the code reconstructs a clean FleetPars object containing the estimated fishing mortality (F/M) ratio and selectivity parameters. # Extract pooled optimization results testOpt &lt;- results_all$pooled # Create clean FleetPars object with estimated parameters FleetPars &lt;- list(FM = testOpt$lbPars[[&quot;F/M&quot;]], selectivityCurve = testOpt$fixedFleetPars$selectivityCurve) # Preserve fleet-specific parameters from original optimization # These lines ensure that extra fields used during selectivity modeling (like mesh sizes or whether aggregation is FALSE or TRUE are preserved if (!is.null(testOpt$fixedFleetPars$SLmesh)) FleetPars$SLmesh &lt;- testOpt$fixedFleetPars$SLmesh if (!is.null(testOpt$fixedFleetPars$SLMin)) FleetPars$SLMin &lt;- testOpt$fixedFleetPars$SLMin if (!is.null(testOpt$fixedFleetPars$use_aggregated)) FleetPars$use_aggregated &lt;- testOpt$fixedFleetPars$use_aggregated if (!is.null(testOpt$fixedFleetPars$fishery_mesh)) FleetPars$fishery_mesh &lt;- testOpt$fixedFleetPars$fishery_mesh # Insert the appropiate selectivity parameters - handle specific selectivity curve parameters if (testOpt$fixedFleetPars$selectivityCurve == &quot;Logistic&quot;) { # For Logistic, parameters might be estimated FleetPars$SL1 &lt;- ifelse(&quot;SL50&quot; %in% names(testOpt$lbPars), testOpt$lbPars[[&quot;SL50&quot;]], testOpt$fixedFleetPars[[&quot;SL1&quot;]]) FleetPars$SL2 &lt;- ifelse(&quot;SL95&quot; %in% names(testOpt$lbPars), testOpt$lbPars[[&quot;SL95&quot;]], testOpt$fixedFleetPars[[&quot;SL2&quot;]]) } else if (testOpt$fixedFleetPars$selectivityCurve == &quot;Knife&quot;) { FleetPars$MLLKnife &lt;- testOpt$fixedFleetPars$MLLKnife } else if (testOpt$fixedFleetPars$selectivityCurve %in% c(&quot;binorm.sca&quot;, &quot;bilognorm&quot;)) { # For new dome-shaped curves with 5 parameters FleetPars$SL1 &lt;- testOpt$fixedFleetPars$SL1 FleetPars$SL2 &lt;- testOpt$fixedFleetPars$SL2 FleetPars$SL3 &lt;- testOpt$fixedFleetPars$SL3 FleetPars$SL4 &lt;- testOpt$fixedFleetPars$SL4 FleetPars$SL5 &lt;- testOpt$fixedFleetPars$SL5 } else { # For other curves (Normal.sca, Normal.loc, logNorm) FleetPars$SL1 &lt;- testOpt$fixedFleetPars$SL1 FleetPars$SL2 &lt;- testOpt$fixedFleetPars$SL2 } The next code re-run the GTG simulation using the estimated parameters. This generates a fresh simulation of the expected equilibrium catch-per-recruit distribution, which is then scaled to match the magnitude of the observed data. The generated plot shows whether the estimated parameters, when used in an independent simulation, can reproduce realistic catch distributions that match the observed data patterns. # This part does the per-recruit simulation using the fitted fishing mortality and selectivity parameters, and then summarizes the expected catch per recruit. prSim &lt;- GTGDomeLBSPRSim2(LifeHistoryObj, FleetPars, SizeBins) sum(prSim$LCatchFished) ## [1] 1 # Scale predicted to match observed range scaled_pred &lt;- max(observed_data) * prSim$LCatchFished / max(prSim$LCatchFished) # Create validation comparison plot p2 &lt;- ggplot(data.frame(Length = length_vector, Observed = observed_data)) + geom_col(aes(x = Length, y = Observed), fill = &quot;grey50&quot;) + geom_line(data = data.frame(Length = LenMids, Predicted = scaled_pred), aes(x = Length, y = Predicted), color = &quot;red&quot;, size = 1.5) + theme_bw() + labs( x = &quot;Length (cm)&quot;, y = &quot;Catch in Number&quot;, title = paste(&quot;Observed vs GTG Re-simulation (&quot;, data_type, &quot;data)&quot;) ) print(p2) Figure 6.14: Observed vs GTG Re-simulation The final plot provides a refined presentation of the optimization fit with improved formatting and clear legend distinction between observed and predicted values. p3 &lt;- ggplot(compare_df, aes(x = Length)) + geom_bar(aes(y = Observed, fill = &quot;Observed&quot;), stat = &quot;identity&quot;, alpha = 0.6) + geom_line(aes(y = Predicted, color = &quot;Predicted&quot;), size = 1.2) + scale_fill_manual(values = c(&quot;Observed&quot; = &quot;gray&quot;)) + scale_color_manual(values = c(&quot;Predicted&quot; = &quot;red&quot;)) + labs(title = paste(&quot;Model Fit Comparison (&quot;, data_type, &quot;data)&quot;), x = &quot;Length (cm)&quot;, y = &quot;Counts&quot;, fill = &quot;&quot;, color = &quot;&quot;) + theme_bw() print(p3) Figure 6.15: Final model fit 6.2.1.12 Step 12. Sensitivity analysis: Multiple life history parameter combinations This section demonstrates how to conduct a comprehensive sensitivity analysis by running the dome-shaped LBSPR model across multiple combinations of key life history parameters. The sensitivity analysis examines the response of the model to variations in two critical life history parameters: M/K ratio and asymptotic length # Conducting the Dome-shaped LB-SPR: apply to multiple M/K and Linf values # Define parameter sequences for sensitivity analysis MKseq &lt;- c(1.5, 1.8, 2.0) # M/K ratios to test Linfseq &lt;- c(90, 100, 120) # asymptotic lengths to test (cm) # Initialize storage for results results_list &lt;- list() row_labels &lt;- c() combo_id &lt;- 1 # counter for parameter combinations The analysis uses a loop structure to systematically evaluate all possible combinations of the specified parameter values. For each combination, the model creates a temporary copy of the life history object with updated parameters and runs the optimization routine using the DoOptDome.aggregated() function. This sensitivity analysis employs DoOptDome.aggregated(), which is a convenience wrapper function that automatically pools (aggregates) multiple groups before optimization. This approach differs from the run_grouped_and_pooled() function used in the previous step, which can perform both grouped and pooled analyses. The DoOptDome.aggregated() function simplifies the workflow for sensitivity analyses where only pooled results are needed across parameter combinations. # Systematic evaluation of parameter combinations for (iLinf in seq_along(Linfseq)) { for (iMK in seq_along(MKseq)) { # Create temporary life history object with updated parameters LifeHistoryObj_temp &lt;- LifeHistoryObj LifeHistoryObj_temp@Linf &lt;- Linfseq[iLinf] LifeHistoryObj_temp@MK &lt;- MKseq[iMK] LifeHistoryObj_temp@M &lt;- MKseq[iMK] * LifeHistoryObj_temp@K # Update M accordingly # Run optimization for current parameter combination using DoOptDome.aggregated() # DoOptDome.aggregated() is a convenience wrapper that pools all groups before optimization runOpt &lt;- DoOptDome.aggregated(LifeHistoryObj_temp, FleetPars, lengthComp, SizeBins = SizeBins, Lc = 0, mod = &quot;GTG&quot;) # Extract key results fm &lt;- runOpt$lbPars[&quot;F/M&quot;] # Fishing mortality ratio spr &lt;- runOpt$lbPars[&quot;SPR&quot;] # Spawning potential ratio nll &lt;- runOpt$NLL # Negative log-likelihood # Calculate confidence intervals when standard errors are available if (!is.null(runOpt$lbStdErrs) &amp;&amp; &quot;F/M&quot; %in% names(runOpt$lbStdErrs)) { fm_se &lt;- runOpt$lbStdErrs[&quot;F/M&quot;] fm_lower &lt;- fm - 1.96 * fm_se # 95% confidence interval fm_upper &lt;- fm + 1.96 * fm_se } else { fm_se &lt;- NA fm_lower &lt;- NA fm_upper &lt;- NA warning(paste(&quot;F/M SE not available for Linf =&quot;, Linfseq[iLinf], &quot;and MK =&quot;, MKseq[iMK])) } # Store results for current combination result_row &lt;- data.frame( MK = MKseq[iMK], Linf = Linfseq[iLinf], `F/M` = fm, `F/M_SE` = fm_se, SPR = spr, NLL = nll, F_M_Lower = fm_lower, F_M_Upper = fm_upper, stringsAsFactors = FALSE ) results_list[[combo_id]] &lt;- result_row row_labels &lt;- c(row_labels, paste0(&quot;combination_&quot;, combo_id)) combo_id &lt;- combo_id + 1 # Progress reporting cat(paste(&quot;Completed: Linf =&quot;, Linfseq[iLinf], &quot;, MK =&quot;, MKseq[iMK], &quot;, F/M =&quot;, round(fm, 4), &quot;, SPR =&quot;, round(spr, 4), &quot;\\n&quot;)) } } ## Completed: Linf = 90 , MK = 1.5 , F/M = 0.0637 , SPR = 0.9363 ## Completed: Linf = 90 , MK = 1.8 , F/M = 0 , SPR = 1 ## Completed: Linf = 90 , MK = 2 , F/M = 0 , SPR = 1 ## Completed: Linf = 100 , MK = 1.5 , F/M = 0.3678 , SPR = 0.7393 ## Completed: Linf = 100 , MK = 1.8 , F/M = 0.0936 , SPR = 0.9152 ## Completed: Linf = 100 , MK = 2 , F/M = 0 , SPR = 1 ## Completed: Linf = 120 , MK = 1.5 , F/M = 0.9146 , SPR = 0.5916 ## Completed: Linf = 120 , MK = 1.8 , F/M = 0.5487 , SPR = 0.6917 ## Completed: Linf = 120 , MK = 2 , F/M = 0.3662 , SPR = 0.7642 After completing all parameter combinations, the results are compiled into a data frame and formatted for a table. # Final data frame results_df &lt;- do.call(rbind, results_list) rownames(results_df) &lt;- row_labels # View it print(results_df) ## MK Linf F.M F.M_SE SPR ## combination_1 1.5 90 6.370840e-02 0.20012166 0.9363187 ## combination_2 1.8 90 2.061160e-09 0.08766016 1.0000000 ## combination_3 2.0 90 2.061161e-09 0.08119598 1.0000000 ## combination_4 1.5 100 3.678265e-01 0.11459528 0.7393483 ## combination_5 1.8 100 9.357802e-02 0.19208825 0.9152235 ## combination_6 2.0 100 2.061172e-09 0.13159961 1.0000000 ## combination_7 1.5 120 9.145970e-01 0.11458780 0.5915958 ## combination_8 1.8 120 5.487217e-01 0.12419010 0.6916688 ## combination_9 2.0 120 3.661990e-01 0.13747066 0.7642024 ## NLL F_M_Lower F_M_Upper ## combination_1 36.78668 -0.32853005 0.4559469 ## combination_2 47.01246 -0.17181392 0.1718139 ## combination_3 64.87243 -0.15914412 0.1591441 ## combination_4 31.36789 0.14321979 0.5924333 ## combination_5 32.09920 -0.28291494 0.4700710 ## combination_6 33.19826 -0.25793524 0.2579352 ## combination_7 31.61274 0.69000493 1.1391891 ## combination_8 31.20558 0.30530913 0.7921343 ## combination_9 31.05859 0.09675652 0.6356415 # Create a table for visualization purposes # Clean up column names and remove duplicates # Create a clean table with proper column order and names visual_table &lt;- data.frame( MK = results_df$MK, Linf = results_df$Linf, `F/M` = round(as.numeric(results_df$F.M), 3), `F/M_SE` = round(as.numeric(results_df$F.M_SE), 3), `F/M_Lower` = round(results_df$F_M_Lower, 3), `F/M_Upper` = round(results_df$F_M_Upper, 3), SPR = round(results_df$SPR, 3), NLL = round(results_df$NLL, 2), check.names = FALSE # This preserves the &quot;/&quot; in column names ) # Print table knitr::kable(visual_table, caption = &quot;GTG dome shaped LBSPR results across MK and Linf&quot;, digits = 2) Table 6.4: GTG dome shaped LBSPR results across MK and Linf MK Linf F/M F/M_SE F/M_Lower F/M_Upper SPR NLL 1.5 90 0.06 0.20 -0.33 0.46 0.94 36.79 1.8 90 0.00 0.09 -0.17 0.17 1.00 47.01 2.0 90 0.00 0.08 -0.16 0.16 1.00 64.87 1.5 100 0.37 0.12 0.14 0.59 0.74 31.37 1.8 100 0.09 0.19 -0.28 0.47 0.92 32.10 2.0 100 0.00 0.13 -0.26 0.26 1.00 33.20 1.5 120 0.92 0.12 0.69 1.14 0.59 31.61 1.8 120 0.55 0.12 0.30 0.79 0.69 31.21 2.0 120 0.37 0.14 0.10 0.64 0.76 31.06 6.2.1.13 Step 13. Optional: logistic selectivity If the user wants to test logistic selectivity and estimate its parameters (e.g., SL1 = SL50 and SL2 = SL95), they should follow a simplified workflow. The user can run the initial part of the script up to the definition of FleetPars for the logistic model. The selectivity fitting step using fit_gillnet_dome() can be skipped, as logistic parameters will be estimated directly from the catch-at-length data using the optimization routine. # Configure LengthComp object for length data lengthComp &lt;- new(&quot;LengthComp&quot;) lengthComp@dataType &lt;- &quot;Frequency&quot; # or &quot;Length&quot; or &quot;Frequency&quot; lengthComp@L_source &lt;- &quot;FD&quot; lengthComp@dt &lt;- sampleCatch lengthComp@L_units &lt;- &quot;cm&quot; lengthComp@L_type &lt;- &quot;TL&quot; lengthComp@header &lt;-TRUE lengthComp@observationGroup &lt;- &quot;Catch&quot; dataType &lt;- &quot;Frequency&quot; # Create LifeHistory S4 object using direct slot assignment LifeHistoryObj &lt;- new(&quot;LifeHistory&quot;) LifeHistoryObj@title &lt;- &quot;Fish Stock&quot; LifeHistoryObj@speciesName &lt;- &quot;Generic Fish&quot; LifeHistoryObj@shortDescription &lt;- &quot;Stock for LBSPR Analysis&quot; LifeHistoryObj@L_type &lt;- &quot;TL&quot; LifeHistoryObj@L_units &lt;- &quot;cm&quot; LifeHistoryObj@Walpha_units &lt;- &quot;g&quot; #Life history LifeHistoryObj@Linf &lt;- 120 LifeHistoryObj@K &lt;- 0.2 # Calculated from your MK ratio (M/K = 1.5, so K = M/1.5) LifeHistoryObj@t0 &lt;- 0 LifeHistoryObj@L50 &lt;- 60 LifeHistoryObj@L95delta &lt;- 2 # L95 - L50 = 62 - 60 LifeHistoryObj@M &lt;- 0.3 # Calculated from MK * K = 1.5 * 0.2 LifeHistoryObj@MK &lt;- 1.5 LifeHistoryObj@LW_A &lt;- 0.01 LifeHistoryObj@LW_B &lt;- 3 LifeHistoryObj@Steep &lt;- 0.7 LifeHistoryObj@R0 &lt;- 1E6 LifeHistoryObj@Tmax &lt;- -log(0.01) / LifeHistoryObj@M # Additional parameters for spawning (set defaults) LifeHistoryObj@recSD &lt;- 0.6 LifeHistoryObj@recRho &lt;- 0 LifeHistoryObj@isHermaph &lt;- FALSE LifeHistoryObj@H50 &lt;- 0 LifeHistoryObj@H95delta &lt;- 0 # GTG-specific parameters as attributes attr(LifeHistoryObj, &quot;CVLinf&quot;) &lt;- 0.1 attr(LifeHistoryObj, &quot;MaxSD&quot;) &lt;- 2 attr(LifeHistoryObj, &quot;NGTG&quot;) &lt;- 13 attr(LifeHistoryObj, &quot;Mpow&quot;) &lt;- 0 attr(LifeHistoryObj, &quot;FecB&quot;) &lt;- 3 # Discretization of length data lengthBinWidth &lt;- 1 SizeBins &lt;- list() SizeBins$Linc &lt;- lengthBinWidth SDLinf &lt;- attr(LifeHistoryObj, &quot;CVLinf&quot;) * LifeHistoryObj@Linf SizeBins$ToSize &lt;- LifeHistoryObj@Linf + SDLinf * attr(LifeHistoryObj, &quot;MaxSD&quot;) LenBins &lt;- seq(from = 0, to = SizeBins$ToSize, by = SizeBins$Linc) LenMids &lt;- seq(from=0.5*SizeBins$Linc, by=SizeBins$Linc, length.out=(length(LenBins)-1)) lengthFish &lt;- seq(from=0.5*SizeBins$Linc, by=SizeBins$Linc, length.out=length(LenBins) - 1) # Check the data structure print(&quot;Data structure:&quot;) ## [1] &quot;Data structure:&quot; print(head(sampleCatch)) ## Length Catch_1 Catch_2 Catch_3 Catch_4 Catch_5 ## 1 0.5 0 0 0 0 0 ## 2 1.5 0 0 0 0 0 ## 3 2.5 0 0 0 0 0 ## 4 3.5 0 0 0 0 0 ## 5 4.5 0 0 0 0 0 ## 6 5.5 0 0 0 0 0 print(&quot;Column names:&quot;) ## [1] &quot;Column names:&quot; print(colnames(sampleCatch)) ## [1] &quot;Length&quot; &quot;Catch_1&quot; &quot;Catch_2&quot; &quot;Catch_3&quot; &quot;Catch_4&quot; ## [6] &quot;Catch_5&quot; # Define fleet parameters for logistic selectivity # Note: SL50 and SL95 parameters will be estimated FleetParsLogistic &lt;- list(selectivityCurve = &quot;Logistic&quot;) The optimization process estimates three key parameters simultaneously: - F/M: Fishing mortality to natural mortality ratio - SL50: Length at 50% selectivity - SL95: Length at 95% selectivity # Run both grouped and pooled optimization # using try to catch erros and prevent the code form crashing results_logistic &lt;- try( run_grouped_and_pooled( lifeHistoryObj = LifeHistoryObj, fixedFleetPars = FleetParsLogistic, LengthCompObj = lengthComp, SizeBins = SizeBins, Lc = 0, mod = &quot;GTG&quot; ), silent = FALSE #No error message printed to console ) ## Running optimization by group... ## Processing group: Catch_1 ## Processing group: Catch_2 ## Processing group: Catch_3 ## Processing group: Catch_4 ## Processing group: Catch_5 ## Running pooled optimization... # For backward compatibility, extract the pooled result # inherits(results_logistic, &quot;try-error&quot;): # Returns TRUE if results_logistic is an error object # Returns FALSE if results_logistic is a normal result lbsprFullLogisticEstimate &lt;- if (!inherits(results_logistic, &quot;try-error&quot;)) { results_logistic$pooled # If optimization succeeded, extract pooled results } else { results_logistic # If optimization failed, keep the error object } # Handle results and potential errors if (inherits(results_logistic, &quot;try-error&quot;)) { cat(&quot;Error occurred:\\n&quot;) print(attr(results_logistic, &quot;condition&quot;)) warning(&quot;Logistic selectivity estimation failed.&quot;) } else { cat(&quot;Optimization successful!\\n&quot;) # Print POOLED results cat(&quot;\\n=== POOLED (AGGREGATED) RESULTS ===\\n&quot;) print(&quot;Estimated parameters:&quot;) print(results_logistic$pooled$lbPars) # Print GROUPED results (if available) if (!is.null(results_logistic$grouped$group_results)) { cat(&quot;\\n=== GROUPED RESULTS (by vector) ===\\n&quot;) for(i in seq_along(results_logistic$grouped$group_results)) { group_name &lt;- names(results_logistic$grouped$group_results)[i] if(is.null(group_name)) group_name &lt;- paste(&quot;Group&quot;, i) cat(paste(&quot;\\n--- Group:&quot;, group_name, &quot;---\\n&quot;)) print(results_logistic$grouped$group_results[[i]]$lbPars) } } else { cat(&quot;\\n=== No grouped results (single vector or all data pooled) ===\\n&quot;) } # Use pooled results for plotting (as before) lbsprFullLogisticEstimate &lt;- results_logistic$pooled # Extract estimated parameters est_FM &lt;- lbsprFullLogisticEstimate$lbPars[&quot;F/M&quot;] est_SL50 &lt;- lbsprFullLogisticEstimate$lbPars[&quot;SL50&quot;] est_SL95 &lt;- lbsprFullLogisticEstimate$lbPars[&quot;SL95&quot;] est_SPR &lt;- lbsprFullLogisticEstimate$lbPars[&quot;SPR&quot;] cat(paste(&quot;F/M =&quot;, round(est_FM, 3), &quot;\\n&quot;)) cat(paste(&quot;SL50 =&quot;, round(est_SL50, 1), &quot;cm\\n&quot;)) cat(paste(&quot;SL95 =&quot;, round(est_SL95, 1), &quot;cm\\n&quot;)) cat(paste(&quot;SPR =&quot;, round(est_SPR, 3), &quot;\\n&quot;)) # Calculate selectivity curves estSelLen &lt;- 1 / (1 + exp(-log(19) * (lengthFish - est_SL50) / (est_SL95 - est_SL50))) # For comparison, let&#39;s also simulate with the estimated parameters FleetParsEstimated &lt;- list( selectivityCurve = &quot;Logistic&quot;, FM = est_FM, SL1 = est_SL50, SL2 = est_SL95 ) # Run simulation with estimated parameters using S4 LifeHistory object sim_result &lt;- GTGDomeLBSPRSim2(LifeHistoryObj, FleetParsEstimated, SizeBins) simSelLen &lt;- sim_result$SelLen[1:length(lengthFish)] # Trim to match length # Create comparison dataframe with different curves compare_df &lt;- data.frame( Length = lengthFish, Calculated = estSelLen, # Calculated from logistic formula Simulated = simSelLen # From GTG simulation ) # Plot comparison p_logistic &lt;- ggplot(compare_df, aes(x = Length)) + geom_line(aes(y = Calculated, color = &quot;Calculated Logistic&quot;), size = 1.2) + geom_line(aes(y = Simulated, color = &quot;GTG Simulated&quot;), linetype = &quot;dashed&quot;, size = 1.2) + geom_vline(xintercept = est_SL50, color = &quot;red&quot;, linetype = &quot;dotted&quot;, alpha = 0.7) + geom_vline(xintercept = est_SL95, color = &quot;orange&quot;, linetype = &quot;dotted&quot;, alpha = 0.7) + geom_vline(xintercept = LifeHistoryObj@L50, color = &quot;grey&quot;, linetype = &quot;dotted&quot;, alpha = 0.7) + # Changed from StockPars$L50 scale_color_manual(values = c(&quot;Calculated Logistic&quot; = &quot;blue&quot;, &quot;GTG Simulated&quot; = &quot;red&quot;)) + labs(title = &quot;Estimated Logistic Selectivity Curve&quot;, subtitle = paste(&quot;SL50 =&quot;, round(est_SL50, 1), &quot;cm, SL95 =&quot;, round(est_SL95, 1), &quot;cm&quot;), x = &quot;Length (cm)&quot;, y = &quot;Selectivity&quot;, color = &quot;Source&quot;) + theme_bw() + theme(legend.position = &quot;bottom&quot;) print(p_logistic) # Also plot fit to data (change from Freq to length) observed &lt;- if(dataType == &quot;Frequency&quot;) rowSums(sampleCatch[,-1]) else hist(unlist(sampleCatch), breaks = LenBins, plot = FALSE)$counts predicted &lt;- lbsprFullLogisticEstimate$PredLen fit_df &lt;- data.frame( Length = if(dataType == &quot;Frequency&quot;) sampleCatch$Length else LenMids, Observed = observed, Predicted = predicted ) p_fit &lt;- ggplot(fit_df, aes(x = Length)) + geom_bar(aes(y = Observed), stat = &quot;identity&quot;, fill = &quot;grey70&quot;, alpha = 0.7) + geom_line(aes(y = Predicted), color = &quot;red&quot;, size = 1.2) + labs(title = &quot;Model Fit: Observed vs Predicted Length Composition&quot;, x = &quot;Length (cm)&quot;, y = &quot;Count&quot;) + theme_bw() print(p_fit) } ## Optimization successful! ## ## === POOLED (AGGREGATED) RESULTS === ## [1] &quot;Estimated parameters:&quot; ## F/M SL50 SL95 SPR ## 3.78636845 57.23855219 66.56190396 0.07708359 ## ## === GROUPED RESULTS (by vector) === ## ## --- Group: Catch_1 --- ## F/M SL50 SL95 SPR ## 3.50681053 56.48644883 64.47376859 0.08064739 ## ## --- Group: Catch_2 --- ## F/M SL50 SL95 SPR ## 3.58527206 57.42913080 67.54093088 0.08349887 ## ## --- Group: Catch_3 --- ## F/M SL50 SL95 SPR ## 4.16087045 57.97326703 67.87565639 0.07244101 ## ## --- Group: Catch_4 --- ## F/M SL50 SL95 SPR ## 3.90930605 56.95272354 65.93410816 0.07241548 ## ## --- Group: Catch_5 --- ## F/M SL50 SL95 SPR ## 3.84555801 57.45438435 67.06150396 0.07678149 ## F/M = 3.786 ## SL50 = 57.2 cm ## SL95 = 66.6 cm ## SPR = 0.077 References Hommik, Kristjan, Ciaran Fitzgerald, Finbarr Kelly, and Samuel Shephard. 2020. “Dome-Shaped Selectivity in LB-SPR: Length-Based Assessment of Data-Limited Inland Fish Stocks Sampled with Gillnets.” Fisheries Research 229: 105574. Millar, Russell B, and Rolf Holst. 1997. “Estimation of Gillnet and Hook Selectivity Using Log-Linear Models.” ICES Journal of Marine Science 54 (3): 471–77. https://doi.org/10.1006/jmsc.1996.0194. "],["references.html", "References", " References Beverton, R. J. H. 1992. “Patterns of Reproductive Strategy Parameters in Some Marine Teleost Fishes.” Journal of Fish Biology 41 (sB): 137–60. https://doi.org/10.1111/j.1095-8649.1992.tb03875.x. Coscino, Connor L., Lyall Bellquist, William J. Harford, and Brice X. Semmens. 2024. “Influence of Life History Characteristics on Data-Limited Stock Status Assertions and Minimum Size Limit Evaluations Using Length-Based Spawning Potential Ratio (LBSPR).” Fisheries Research 276 (August): 107036. https://doi.org/10.1016/j.fishres.2024.107036. Froese, Rainer. 2004. “Keep It Simple: Three Indicators to Deal with Overfishing.” Fish and Fisheries 5 (1): 86–91. https://doi.org/10.1111/j.1467-2979.2004.00144.x. Harford, W. 2024. “fishSimGTG: Numerical Simulations of Fish Population Dynamics. R Package Version 1.0.6.” https://github.com/natureanalytics-ca/fishSimGTG. Hommik, Kristjan, Ciaran Fitzgerald, Finbarr Kelly, and Samuel Shephard. 2020. “Dome-Shaped Selectivity in LB-SPR: Length-Based Assessment of Data-Limited Inland Fish Stocks Sampled with Gillnets.” Fisheries Research 229: 105574. Hordyk, A. 2021. “LBSPR: Length-Based Spawning Potential Ratio. R Package Version 0.1.6.” https://github.com/AdrianHordyk/LBSPR. Hordyk, Adrian R., Kotaro Ono, Jeremy D. Prince, and Carl J. Walters. 2016a. “A Simple Length-Structured Model Based on Life History Ratios and Incorporating Size-Dependent Selectivity: Application to Spawning Potential Ratios for Data-Poor Stocks.” Canadian Journal of Fisheries and Aquatic Sciences 73 (12): 1787–99. https://doi.org/10.1139/cjfas-2015-0422. Hordyk, Adrian R, Kotaro Ono, Jeremy D Prince, and Carl J Walters. 2016b. “A Simple Length-Structured Model Based on Life History Ratios and Incorporating Size-Dependent Selectivity: Application to Spawning Potential Ratios for Data-Poor Stocks.” Canadian Journal of Fisheries and Aquatic Sciences 73 (12): 1787–99. https://doi.org/10.1139/cjfas-2015-0422. Mildenberger, Tobias K., Michael H. Taylor, and Matthias Wolff. 2017. “TropFishR: An r Package for Fisheries Analysis with Length-Frequency Data.” Methods in Ecology and Evolution 8 (11): 1520–27. Millar, Russell B, and Rolf Holst. 1997. “Estimation of Gillnet and Hook Selectivity Using Log-Linear Models.” ICES Journal of Marine Science 54 (3): 471–77. https://doi.org/10.1006/jmsc.1996.0194. "]]
