[["index.html", "fishLengthAssess R package 1 What is fishLengthAssess? 1.1 Installation 1.2 How to use fishLengthAssess?", " fishLengthAssess R package William Harford 2025-06-02 1 What is fishLengthAssess? fishLengthAssess acts as an aggregator of length-based stock assessment models. The package imposes a standardized formatting and metadata structure on length data sets, which can then be used in a variety of length-based assessment methods that are aggregated within the package. It also provides wrapper functions to link standardized formatting of length data sets with external R packages that contain length-based assessment methods (e.g. LBSPR package). 1.1 Installation fishLengthAssess depends on other packages to run its functions. These are the devtools, fishSimGTG (Harford 2024) and LBSPR (A. Hordyk 2021) R packages, that should be installed as follows: install.packages(&quot;devtools&quot;) install.packages(&quot;LBSPR&quot;) devtools::install_github(&quot;natureanalytics-ca/fishSimGTG@v1.0.6&quot;) The user then can install the development version of fishLengthAssess from GitHub with: devtools::install_github(&quot;natureanalytics-ca/fishLengthAssess&quot;) 1.2 How to use fishLengthAssess? The fishLengthAssess package aggregates data-limited methods that can be used for evaluating the condition of a fish stock based on length data sets. The methods are split in two categories: indicator-based functions and model-based functions. Indicator-based functions are based on metrics that include compliance with established size limits, 3 metrics proposed by Froese (2004), and trend-based metrics like mean length in the catch. Model-based methods consist of length-based stock assessment approaches, and includes an interface for estimation of SPR from observed length-frequency data, based on the LB-SPR method developed by Adrian R. Hordyk et al. (2016a). The main functions in the package rely on life histories and length composition objects (LifeHistoryObj and LengthCompObj, respectively). Once a user standardizes their length data to fit the requirements of the length composition object and gathers the necessary information for creating a life history object, then all the functions within the fishLengthAssess package are straightforward to use having these objects as function arguments. The next chapter describes in more detail how to structure the data into the necessary R objects. References Froese, Rainer. 2004. “Keep It Simple: Three Indicators to Deal with Overfishing.” Fish and Fisheries 5 (1): 86–91. https://doi.org/10.1111/j.1467-2979.2004.00144.x. Harford, W. 2024. “fishSimGTG: Numerical Simulations of Fish Population Dynamics. R Package Version 1.0.6.” https://github.com/natureanalytics-ca/fishSimGTG. Hordyk, A. 2021. “LBSPR: Length-Based Spawning Potential Ratio. R Package Version 0.1.6.” https://github.com/AdrianHordyk/LBSPR. Hordyk, Adrian R., Kotaro Ono, Jeremy D. Prince, and Carl J. Walters. 2016a. “A Simple Length-Structured Model Based on Life History Ratios and Incorporating Size-Dependent Selectivity: Application to Spawning Potential Ratios for Data-Poor Stocks.” Canadian Journal of Fisheries and Aquatic Sciences 73 (12): 1787–99. https://doi.org/10.1139/cjfas-2015-0422. "],["r-objects-and-example-data-sets.html", "2 R objects and example data sets", " 2 R objects and example data sets The main functions in the package rely on life histories and length composition objects. The life history object LifeHistoryObj is an S4 object that holds the description of a life history. It gathers information about life history traits of each species such as the parameters of the length-weight relationship, von Bertalanffy growth parameters, length at 50% maturity, natural mortality, and others. The LifeHistoryObj belongs to the class LifeHistory in the fishSimGTG R package. An example LifeHistoryObj can be accessed within the fishLengthAssess package (LifeHistoryExample) and the user can see the elements or slots of the LifeHistoryObj using the slotNames() function. # Load the package library(fishLengthAssess) # use function slotNames to see elements of LifeHistoryObj example slotNames(fishSimGTG::LifeHistoryExample) ## [1] &quot;title&quot; &quot;speciesName&quot; &quot;shortDescription&quot; &quot;L_type&quot; ## [5] &quot;L_units&quot; &quot;Walpha_units&quot; &quot;Linf&quot; &quot;K&quot; ## [9] &quot;t0&quot; &quot;L50&quot; &quot;L95delta&quot; &quot;M&quot; ## [13] &quot;MK&quot; &quot;LW_A&quot; &quot;LW_B&quot; &quot;Tmax&quot; ## [17] &quot;Steep&quot; &quot;R0&quot; &quot;recSD&quot; &quot;recRho&quot; ## [21] &quot;isHermaph&quot; &quot;H50&quot; &quot;H95delta&quot; # check what LifeHistoryExample contains fishSimGTG::LifeHistoryExample ## An object of class &quot;LifeHistory&quot; ## Slot &quot;title&quot;: ## [1] &quot;Example fish&quot; ## ## Slot &quot;speciesName&quot;: ## [1] &quot;Example fish&quot; ## ## Slot &quot;shortDescription&quot;: ## [1] &quot;Simulated life history of a fish based on B-H invariants&quot; ## ## Slot &quot;L_type&quot;: ## [1] &quot;TL&quot; ## ## Slot &quot;L_units&quot;: ## [1] &quot;cm&quot; ## ## Slot &quot;Walpha_units&quot;: ## [1] &quot;g&quot; ## ## Slot &quot;Linf&quot;: ## [1] 100 ## ## Slot &quot;K&quot;: ## [1] 0.2 ## ## Slot &quot;t0&quot;: ## [1] 0 ## ## Slot &quot;L50&quot;: ## [1] 66 ## ## Slot &quot;L95delta&quot;: ## [1] 1 ## ## Slot &quot;M&quot;: ## [1] 0.3 ## ## Slot &quot;MK&quot;: ## [1] 1.5 ## ## Slot &quot;LW_A&quot;: ## [1] 0.01 ## ## Slot &quot;LW_B&quot;: ## [1] 3 ## ## Slot &quot;Tmax&quot;: ## [1] 15 ## ## Slot &quot;Steep&quot;: ## [1] 0.99 ## ## Slot &quot;R0&quot;: ## [1] 1000 ## ## Slot &quot;recSD&quot;: ## [1] 0.6 ## ## Slot &quot;recRho&quot;: ## [1] 0 ## ## Slot &quot;isHermaph&quot;: ## [1] FALSE ## ## Slot &quot;H50&quot;: ## numeric(0) ## ## Slot &quot;H95delta&quot;: ## numeric(0) The values of the slots can be edited by the user using the @ symbol. # store LifeHistoryExample as a variable LifeHistory_example &lt;- fishSimGTG::LifeHistoryExample # change slot speciesName LifeHistory_example@speciesName &lt;- &quot;Example species&quot; LifeHistory_example@speciesName ## [1] &quot;Example species&quot; More details on the life history object can be found in the user guide of the fishSimGTG R package in here. The length composition object is a length data set that can be structured either as raw data (i.e. individual length measurements) or length frequency data (i.e. numbers of fish per length bin). The raw length data is a collection of length measurements stored as a vector. These are typically original length measurements of fish, which have not been binned. Multiple columns can be used, with each column pertaining to a level of a grouping variable, such as year or fleet. The length frequency data is a collection of length measurements organized using two or more columns. The left-most column must contain bin mid points. The next column contains the number of length measurement observations in each bin. While the first column is reserved for the bin mid points, multiple columns to its right can be used with each pertaining to a level of a grouping variable, such as year or fleet. # Example data set with length composition defined as length frequencies by year knitr::kable(LengthCompExampleFreq@dt[21:30,],row.names = F) LMids 2015 2016 2017 2018 2019 51 0 0 1 16 31 53 0 2 2 22 37 55 3 0 1 40 25 57 2 3 1 34 20 59 1 8 5 41 14 61 5 9 7 42 11 63 7 8 10 33 10 65 11 12 24 30 5 67 12 13 25 12 6 69 13 26 25 18 3 # Example data set with length composition defined as raw length measurements by year knitr::kable(head(LengthCompExampleLength@dt)) 2015 2016 2017 2018 2019 55 49 51 43 29 55 53 53 43 31 55 53 53 45 31 57 57 55 45 33 57 57 57 45 33 59 57 59 47 33 "],["indicator-based-functions.html", "3 Indicator-based functions 3.1 PmatFunc function 3.2 PoptFunc function 3.3 PmegaFunc function 3.4 PLcFunc function", " 3 Indicator-based functions 3.1 PmatFunc function The PmatFunc function calculates the proportion of the catch that is above length at 50% maturity (L50). This metric is calculated relative to the length at which 50% of fish are mature, which serves as a threshold to determine maturity. The goal for mature fish in the catch is 100%, as it is generally recommended to allow fish to reproduce at least once before becoming vulnerable to fishing (Froese 2004). Mature fish in the catch is calculated as the percentage of the length composition that is equal to or greater than L50. 3.2 PoptFunc function The PoptFunc function calculates the proportion of fish within optimal sizes in the catch. Optimal size refers to the length at which a fish cohort achieves its maximum biomass, and thus, fishing sizes close to this optimal size should ensure high yields by weight. This function relies on a utility function LoptFunc that calculates the optimal length in the catch (optimum harvest length; Beverton (1992)). The range of optimal lengths in the catch is then defined as \\(Lopt\\) +/- 10%. The percent optimal sizes in the catch is calculated as the percentage of the length composition that fall within the defined range. 3.3 PmegaFunc function The PmegaFunc function calculates the proportion of mega-spawners in the catch. Mega-spawners are those larger than the optimal size range, thus they should comprise a low percentage of the catch. Percent mega-spawners is calculated as the percentage of the length composition equal or greater than optimum length plus 10%. Interpretation of this metric is nuanced because while it may be desirable to leave mega-spawners in the water, if mega-spawners are not present in the catch but are not being intentionally avoided by fishers, a lack of mega-spawners could be an indication of overfishing. For this reason, Froese (2004) suggests a target of 30% to 40% of mega-spawners in the catch. Conversely, if mega-spawners are being avoided due to market preferences, gear selectivity or through other intentional interventions, such as slot limits, this can allow larger-older fish spawn and contribute to population replenishment. 3.4 PLcFunc function The PLcFunc function calculates the proportion of catch above the minimum size limit. For fisheries with size limits, fish above the size limit is calculated as the percentage of the length composition that are equal or greater than the size limit. References Beverton, R. J. H. 1992. “Patterns of Reproductive Strategy Parameters in Some Marine Teleost Fishes.” Journal of Fish Biology 41 (sB): 137–60. https://doi.org/10.1111/j.1095-8649.1992.tb03875.x. Froese, Rainer. 2004. “Keep It Simple: Three Indicators to Deal with Overfishing.” Fish and Fisheries 5 (1): 86–91. https://doi.org/10.1111/j.1467-2979.2004.00144.x. "],["model-based-functions.html", "4 Model-based functions 4.1 lbsprWrapper function 4.2 GTG Length-Based SPR model including dome-shaped selectivity", " 4 Model-based functions 4.1 lbsprWrapper function The lbsprWrapper function is a wrapper function for conducting length-based stock assessment (LBSPR) using length-based SPR methods (Hordyk et al. 2016). The model can use a conventional age-structured equilibrium population model or a length-structured version that is determined by growth-type-groups to account for size-based selectivity. Length frequency distribution of the stock is determined by natural mortality, fishing mortality, and length-based (LB) vulnerability to fishing. A relationship also exists between reproductive output of the population and length frequency distribution. Accordingly, measurements of the length frequency distribution can be used to infer SPR. The maximum likelihood LB-SPR estimation routine requires inputs of M/K, asymptotic length, a logistic maturity curve, coefficient of variation of asymptotic length and exponential parameter for fecundity. The former three inputs are obtained from a life history. The latter two inputs are assumed as follows: coefficient of variation of asymptotic length is specified at its default value of 0.1 and exponential parameter for fecundity is set equal to the beta parameter of the length-weight relationship or to a value of 3.0 if the beta parameter is unavailable. A length data set is also required, with the following caveats. First, LBSPR bins length data to produce a length-frequency distribution. When a length data set contains raw lengths, a bin width of 1 cm is used. When a length data set contains frequencies, the bin width of the length data set is used in the LBSPR fitting routine. Second, because LBSPR estimates the parameter of logistic selectivity function during the fitting process, the gear used to collect the length data must have asymptotic selectivity. Do not use LBSPR is this assumption cannot be met. Third, LBSPR assumes size structure data are representative at face value, so instances of hyperstability in the length frequency should be treated with caution (Coscino et al. 2024). For more information on the LBSPR model the user can check the package vignette here. 4.2 GTG Length-Based SPR model including dome-shaped selectivity This code is based on the GTG LBSPR framework originally developed by Adrian R. Hordyk et al. (2016b), which implements a growth-type-group (GTG) approach to simulate length-based per-recruit dynamics (https://github.com/AdrianHordyk/GTG_LBSPR). The GTG LBSPR code was extended by Hommik et al. (2020) (https://github.com/KHommik/DomeShaped_GTG_LBSPR) to incorporate dome-shaped selectivity models, specifically normal and log-normal curves, allowing more realistic selectivity patterns for length-based simulations in data-limited contexts. This current version further expands the functionality by integrating selectivity models commonly used in gillnet fisheries, particularly those available in the TropFishR package (Mildenberger, Taylor, and Wolff 2017), including: Normal (common spread) model, Normal (scaled spread) model, and Lognormal model. These three selectivity models are based on the SELECT framework developed by Millar and Holst (1997), which uses log-linear models for fitting gillnet and hook selectivity. Additionally, the code now supports two bimodal selectivity options: bimodal normal with scaled spread and bilognorm — bimodal log-normal. The new configurations allow that the dome-shaped models can be used either with aggregated mesh selectivity (as in Hommik et al. (2020)) or with individual mesh-specific (specific mesh size) selectivity. In addition the new code accepts different groups of length data providing the user the opportunity to estimate F/M and SPR for each group or for the pooled data. This code implements a length-based assessment approach that accounts for individual variation in growth using the Growth-Type Group (GTG) approach; supports multiple selectivity curves, including dome-shaped options; provides parameter estimation via maximum likelihood; and calculates key fisheries reference points like SPR. The model is part of the R package fishLengthAssess and and consists of several interconnected functions: GTGDomeLBSPRSim2(): The core simulation function that calculates SPR based on life history parameters, fleet parameters, and size bins. This function contains all the biological and mathematical models. processLengthCompData(): Data preprocessing function that converts S4 LengthComp objects into the format required by the optimization functions. Handles both frequency and raw length data with options for grouped or pooled analysis. OptFunDome: The objective function used during parameter estimation. This function takes trial parameter values, runs the simulation (GTGDomeLBSPRSim2()), compares predicted vs observed length compositions, and returns a negative log-likelihood value. DoOptDome: The main optimization function that manages the parameter estimation process (uses optim()). It sets up initial parameter values, calls the optimization algorithm (which repeatedly uses OptFunDome() to evaluate different parameter combinations), and processes the final results including standard errors and model diagnostics. DoOptDome.LengthComp(): It is a user interface function that accepts S4 LengthComp objects and automatically handles data processing before calling the core optimization functions. It provides options for analyzing multiple groups separately (byGroup = TRUE) or combining them into a single analysis (byGroup = FALSE). DoOptDome.aggregated(): It is a user interface function that ensures data from multiple groups is always combined (pooled) before analysis, regardless of the original data structure. This is useful when the user wants to analyze the combined dataset. run_grouped_and_pooled(): This function provides a comparative analysis that automatically runs the same dataset through both grouped analysis (each group fitted separately) and pooled analysis (all groups combined), allowing users to compare results and assess whether pooling affects parameter estimates. 4.2.1 Core simulation function: GTGDomeLBSPRSim2 This is the main function of the model. GTGDomeLBSPRSim2() simulates per-recruit population dynamics for fish populations structured by Growth-Type Groups (GTGs), allowing for dome-shaped or logistic selectivity curves. It supports mesh-specific or aggregated selectivity and returns biological reference points such as spawning potential ratio (SPR), yield per recruit (YPR), and equilibrium recruitment. GTGDomeLBSPRSim2 &lt;- function(lifeHistoryObj, FleetPars, SizeBins=NULL) 4.2.1.1 Arguments The arguments of the GTGDomeLBSPRSim2() function are: lifeHistoryObj: S4 life history object containing biological parameters (growth, maturity, etc.). FleetPars: List containing fishery parameters (selectivity, fishing mortality). SizeBins: List defining length bins for analysis. lifeHistoryObj(S4 Object): The life history object (lifeHistoryObj) contains the following slots accessed via @: @Linf: Mean asymptotic length @MK: M/K ratio (natural mortality over growth) @L50: Length at 50% maturity @L95delta: Delta between L95 and L50 maturity (L95 = L50 + L95delta) @LW_A, @LW_B: Weight-at-length coefficients (Walpha, Wbeta) @Steep: Beverton-Holt steepness @R0: Virgin recruitment (can be a large arbitrary number, e.g., 1e6) Additional attributes are added to this lifeHistoryObj via attr(): NGTG: Number of growth-type groups (default = 13) CVLinf: Coefficient of variation in Linf (default = 0.1) MaxSD: Multiplier of SD to define GTG range (default = 2) GTGLinfBy: Increment between GTG Linf values (default = NA) FecB: Fecundity exponent (default = 3) Mpow: Exponent for M/K ratio (default = 0) FleetPars: Required elements: FM: Fishing mortality rate (F/M) selectivityCurve: One of the following supported selectivity curves \"Logistic\": Two-parameter logistic selectivity (SL1: size at length of 50% of selectivity and SL2: size at length of 95% of selectivity). \"Knife\": Binary selectivity above a threshold length \"Normal.loc\": Normal (common spread) model \"Normal.sca\": Normal (scaled spread) model \"lognorm\": Lognormal model \"binorm.sca\": Bi-normal model \"bilognorm\": Bi-lognormal model A complete description of dome-shaped selectivity curves is provided in Section 5: Utility functions. Selectivity specific parameters: SL1-SL5: Selectivity parameters (see table below) SLmesh: Vector of mesh sizes (for dome-shaped models) SLMin: (Optional) Minimum length selected MLLKnife: Minimum legal length (for “Knife” selectivity) use_aggregated: If TRUE, aggregate selectivity across mesh sizes fishery_mesh: (Optional) mesh size used if not aggregated Selectivity Type SL1 SL2 SL3 SL4 SL5 \"Logistic\" Length at 50% selectivity Length at 95% selectivity — — — \"Normal.loc\" Mode of normal curve Spread (SD) (fixed) — — — \"Normal.sca\" Mode of normal curve Spread (SD) (proportional to mesh) — — — \"logNorm\" Parameter mean in log space Parameter SD in log space — — — \"binorm.sca\" Mode 1 (first peak) SD 1 Mode 2 (second peak) SD 2 Logit(P1): proportion of retention assigned to the first peak \"bilognorm\" Mode 1 (first peak in log space) SD 1 in log space Mode 2 (second peak in log space) SD 2 in log space Logit(P1): proportion of retention assigned to the first peak \"Knife\" — — — — — Note: In bimodal models, SL5 is a logit-transformed parameter that represents the proportion of retention assigned to the first peak. \\[ P_1 = \\frac{\\exp(\\text{SL5})}{1 + \\exp(\\text{SL5})} \\] A complete description of dome-shaped selectivity parameters is provided in Section 5: Utility functions. SizeBins: Required elements: Linc: Bin width (default = 1) ToSize: Upper bound for length classes (default = Linf + MaxSD * SD_Linf) 4.2.1.2 Parameter extraction and setup of GTGDomeLBSPRSim2() # Direct S4 access to lifeHistoryObj NGTG &lt;- attr(lifeHistoryObj, &quot;NGTG&quot;) if (is.null(NGTG)) NGTG &lt;- 13 # Default Linf &lt;- lifeHistoryObj@Linf MK &lt;- lifeHistoryObj@MK L50 &lt;- lifeHistoryObj@L50 L95 &lt;- lifeHistoryObj@L50 + lifeHistoryObj@L95delta # Convert from delta # ... more parameters extracted The function first extracts biological parameters from the S4 life history object lifeHistoryObj, including: Growth parameters (Linf, CV) via direct slot access (@) and attribute access (attr()) Mortality-to-growth ratio (M/K) Maturity parameters (L50, L95delta) Weight-length relationship (LW_A, LW_B) Fecundity parameters 4.2.1.3 Setting up Growth-Type Groups (GTGs) The function creates a range of Linf values centered on the mean Linf and distributes recruits across these groups. This accounts for individual variability in growth. # Set up Linfs for different GTGs if (exists(&quot;NGTG&quot;) &amp; !exists(&quot;GTGLinfBy&quot;)) { DiffLinfs &lt;- seq(from=Linf-MaxSD*SDLinf, to=Linf+MaxSD*SDLinf, length=NGTG) GTGLinfBy &lt;- DiffLinfs[2]-DiffLinfs[1] } else if (!exists(&quot;NGTG&quot;) &amp; exists(&quot;GTGLinfBy&quot;)) { DiffLinfs &lt;- seq(from=Linf-MaxSD*SDLinf, to=Linf+MaxSD*SDLinf, by=GTGLinfBy) NGTG &lt;- length(DiffLinfs) } else if (exists(&quot;NGTG&quot;) &amp; exists(&quot;GTGLinfBy&quot;)) { if (!is.na(GTGLinfBy)) { DiffLinfs &lt;- seq(from=Linf-MaxSD*SDLinf, to=Linf+MaxSD*SDLinf, by=GTGLinfBy) NGTG &lt;- length(DiffLinfs) } if (is.na(GTGLinfBy)) { DiffLinfs &lt;- seq(from=Linf-MaxSD*SDLinf, to=Linf+MaxSD*SDLinf, length=NGTG) GTGLinfBy &lt;- DiffLinfs[2]-DiffLinfs[1] } } 4.2.1.4 Selectivity model implementation A major feature of this model is its ability to use different selectivity curves: if(selectivityCurve==&quot;Logistic&quot;){ VulLen &lt;- 1.0/(1+exp(-log(19)*((LenBins+0.5*Linc)-SL50)/((SL95)-(SL50)))) } else if(selectivityCurve==&quot;Normal.sca&quot;){ # Normal Scale implementation # ... } else if(selectivityCurve==&quot;Normal.loc&quot;){ # Normal Location implementation # ... } # ... more selectivity options The model supports both mesh-aggregated and single-mesh selectivity for dome-shaped curves. 4.2.1.5 Population dynamics simulation The function simulates both unfished and fished populations, tracking numbers, biomass, and reproductive output across length classes and growth-type groups. # Initialize matrices NPRFished &lt;- NPRUnfished &lt;- matrix(0, nrow=length(LenBins), ncol=NGTG) # ... more matrices # Distribute recruits to first length class NPRFished[1, ] &lt;- NPRUnfished[1, ] &lt;- RecProbs * R0 # Calculate numbers at each size class for (L in 2:length(LenBins)) { NPRUnfished[L, ] &lt;- NPRUnfished[L-1, ] * ((DiffLinfs-LenBins[L])/(DiffLinfs-LenBins[L-1]))^MKMat[L-1, ] NPRFished[L, ] &lt;- NPRFished[L-1, ] * ((DiffLinfs-LenBins[L])/(DiffLinfs-LenBins[L-1]))^ZKLMat[L-1, ] # ... more calculations } 4.2.1.6 SPR and Yield calculation The function calculates SPR as the ratio of eggs-per-recruit in the fished vs. unfished population, as well as yield-per-recruit and total yield. # Calculate SPR EPR0 &lt;- sum(NatLUnFishedPop * FecLenGTG) # Eggs-per-recruit Unfished EPRf &lt;- sum(NatLFishedPop * FecLenGTG) # Eggs-per-recruit Fished SPR &lt;- EPRf/EPR0 # Spawning potential ratio # Calculate yield YPR &lt;- sum(NatLFishedPop * Weight * VulLen2) * FM Yield &lt;- YPR * RelRec 4.2.1.7 Main outputs of the GTGDomeLBSPRSim2() function The function returns a comprehensive list of calculated values, including: Element Description SPR Spawning Potential Ratio YPR, Yield Yield per recruit and total yield LCatchFished, LCatchUnfished Normalized length composition of catch (fished and unfished) LPopFished, LPopUnfished Normalized population-at-length (fished and unfished) NatLPopFished, NatLPopUnFish Raw GTG-specific number-at-length matrices (fished and unfished) NatLCatchFish, NatLCatchUnFish GTG-specific catch matrices (fish and unfished) FecLen, MatLen Fecundity and maturity-at-length per GTG SelLen, VulLen2 Selectivity-at-length vectors (bins and mids) ObjFun Fitness deviation metric (for optimization) SPRatsize Cumulative SPR by size class RecProbs, RelRec Recruitment probability and equilibrium recruitment … Other additional diagnostic variables for plotting and analysis 4.2.2 Data Processing Function: processLengthCompData The processLengthCompData() function processes S4 LengthComp objects for use in the GTG dome-shaped LBSPR model, handling both frequency and raw length data with support for grouped or pooled analysis. processLengthCompData(LengthCompObj, byGroup = FALSE, SizeBins = NULL, Lc = 0) 4.2.2.1 Arguments LengthCompObj: S4 LengthComp object containing length data byGroup: Logical, whether to process by group (default FALSE). If TRUE, analyzes each group separately; if FALSE, pools all data together. SizeBins: List with Linc and ToSize elements (optional). If NULL, defaults to 1 cm bins up to reasonable maximum size. Lc: Length at first capture (default 0). For fishery-independent data, must be &gt;= 0. Fish smaller than Lc are removed from analysis. 4.2.2.2 Data processing steps Input Validation: Checks S4 object structure and parameter consistency. Data Extraction: Uses poolLengthComp() to extract data from S4 structure. Binning: For raw length data, creates frequency distributions using hist(). Filtering: Removes fish below Lc for fishery-independent (FI) data. Group Handling: Manages multiple groups based on byGroup argument. 4.2.2.3 Main outputs of the processLengthCompData() function The data processing function processLengthCompData() returns a list containing processed length data: Element Description LenDat Frequency matrix or vector LenMids Length bin midpoints group_names Group identifiers n_groups Number of groups was_pooled Whether data was pooled original_dataType “Frequency” or “Length L_source “FI” or “FD” pooled_data Raw processed data frame binWidth Length bin width used 4.2.3 Optimization function: OptFunDome The OptFunDome() function evaluates how well a proposed set of selectivity and fishing mortality parameters fit observed length-frequency data using a per-recruit simulation and multinomial likelihood. It is used as the objective function in an optimization routine (e.g., within optim()), where the goal is to minimize the negative log-likelihood (NLL) of the predicted length composition relative to observed data. OptFunDome(tryFleetPars, fixedFleetPars, LenDat, lifeHistoryObj, SizeBins = NULL, mod = c(&quot;GTG&quot;, &quot;LBSPR&quot;)) 4.2.3.1 Arguments tryFleetPars: A numeric vector of parameters to estimate (typically log-transformed). For logistic selectivity with estimation, tryFleetPars = c(log(F/M), log(SL50/Linf), log((SL95 - SL50)/Linf)). If logistic selectivity parameters are being estimated, a penalty is added to the objective function if SL50 approaches unrealistic values (i.e., too close to Linf). The penalty is based on a beta distribution. For dome-shaped models (selectivity is fixed)), tryFleetPars = c(log(F/M)). fixedFleetPars: A list of selectivity model parameters to keep fixed during optimization. This must include the selectivityCurve name and any other parameters (e.g., SL1–SL5, SLmesh, etc). LenDat: A vector of observed length-frequency data (number of fish in each length bin). lifeHistoryObj: S4 life history object (same as in GTGDomeLBSPRSim2()) SizeBins: Same as in the GTGDomeLBSPRSim2() function mod: Model type (character); only \"GTG\" is supported (uses growth-type group model). 4.2.3.2 Key steps in the function OptFunDome() The function sets up fleet parameters based on inputs, runs the simulation model, and calculates the negative log-likelihood between observed and predicted length distributions. The main output is a single numeric value that represents the negative log-likelihood (NLL) of the predicted vs. observed length distribution, including a penalty if logistic parameters are being estimated and SL1 approaches unrealistic values (too close to Linf). # Set up fleet parameters Fleet &lt;- NULL Fleet$selectivityCurve &lt;- fixedFleetPars$selectivityCurve # Set selectivity parameters based on curve type if(Fleet$selectivityCurve==&quot;Logistic&quot;){ # ... set logistic parameters } else if(Fleet$selectivityCurve==&quot;Knife&quot;){ # ... set knife-edge parameters } else if(Fleet$selectivityCurve %in% c(&quot;Normal.sca&quot;, &quot;Normal.loc&quot;, &quot;logNorm&quot;, &quot;binorm.sca&quot;,&quot;bilognorm&quot;)){ # ... set dome-shaped parameters } # Set fishing mortality Fleet$FM &lt;- exp(tryFleetPars[1]) # Run the simulation model runMod &lt;- GTGDomeLBSPRSim2(lifeHistoryObj, Fleet, SizeBins) # Calculate negative log-likelihood LenDat &lt;- LenDat + 1E-15 # Add tiny constant to avoid log(0) LenProb &lt;- LenDat/sum(LenDat) # Observed proportions predProb &lt;- runMod$LCatchFished # Predicted proportions predProb &lt;- predProb + 1E-15 # Add tiny constant NLL &lt;- -sum(LenDat * log(predProb/LenProb)) # Negative log-likelihood 4.2.4 Optimization wrapper: DoOptDome This function manages the optimization process to estimate parameters. The function DoOptDome() is the wrapper that runs the full optimization process. It tries to find the best values for fishing mortality (F/M) and, if needed, selectivity parameters like SL1 and SL1 (only for logistic selectivity). To do that, it uses OptFunDome()(i.e., the objective function) to evaluates the likelihood for a given parameter set. This function performs maximum likelihood estimation of fishing mortality and, optionally, selectivity parameters based on observed length-frequency data. It uses a per-recruit simulation (GTGDomeLBSPRSim2()) under the Growth-Type Group (\"GTG\") model framework, and compares observed and predicted length distributions using a multinomial likelihood. 4.2.4.1 Arguments lifeHistoryObj: S4 life history object containing biological parameters fixedFleetPars: A list of fixed fishing/selectivity parameters, including selectivityCurve name, values for SL1–SL5, SLmesh, SLMin, use_aggregated, and fishery_mesh. LenDat: A numeric vector. The observed length-frequency data (e.g., counts per length bin). SizeBins: A list or NULL. A list with Linc (length bin width) and ToSize (maximum length). Defaults are created if not provided. mod: A character. Currently only “GTG” is supported by this code. The function sets up initial parameter values and runs the optimization using either BFGS (when estimating F/M and logistic selectivity) or Brent (when estimating only F/M and dome-shaped selectivity is fixed) methods: 4.2.4.2 Main outputs of the DoOptDome() function The DoOptDome() function returns a list containing: Element Description lbPars Estimated parameters: F/M, SL1 and SL2 (if logistic is estimated), and SPR (derived quantity) lbStdErrs Standard errors of the estimated parameters fixedFleetPars Original (fixed) selectivity settings. PredLen Predicted length-frequency data (expected catch numbers by length bin) NLL Final negative log-likelihood value optimOut Full optim() output object MLE Table of parameter estimates, initial values, and standard errors 4.2.5 User interface functions: DoOptDome.LengthComp Wrapper function for DoOptDome that handles S4 LengthComp objects with support for both grouped and pooled analysis approaches. DoOptDome.LengthComp(lifeHistoryObj, fixedFleetPars, LengthCompObj, SizeBins = NULL, byGroup = FALSE, Lc = 0, mod = c(&quot;GTG&quot;, &quot;LBSPR&quot;)) The DoOptDome.LengthComp function processes S4 LengthComp objects using processLengthCompData(), handles multiple groups either separately (byGroup = TRUE) or pooled (byGroup = FALSE) and returns results appropriate to the analysis approach chosen. DoOptDome.aggregated Convenience wrapper that always pools (aggregates) multiple groups before optimization: DoOptDome.aggregated(lifeHistoryObj, fixedFleetPars, LengthCompObj, SizeBins = NULL, Lc = 0, mod = c(&quot;GTG&quot;, &quot;LBSPR&quot;)) run_grouped_and_pooled Convenience function that runs both grouped and pooled optimization analyses on the same dataset: run_grouped_and_pooled(lifeHistoryObj, fixedFleetPars, LengthCompObj, SizeBins = NULL, Lc = 0, mod = &quot;GTG&quot;) The run_grouped_and_pooled function returns a list with both grouped and pooled results, allowing comparison between approaches. In Chapter 6, the user will find a step-by-step example of applying the GTG Length-Based SPR model, including dome-shaped selectivity. References Coscino, Connor L., Lyall Bellquist, William J. Harford, and Brice X. Semmens. 2024. “Influence of Life History Characteristics on Data-Limited Stock Status Assertions and Minimum Size Limit Evaluations Using Length-Based Spawning Potential Ratio (LBSPR).” Fisheries Research 276 (August): 107036. https://doi.org/10.1016/j.fishres.2024.107036. Hommik, Kristjan, Ciaran Fitzgerald, Finbarr Kelly, and Samuel Shephard. 2020. “Dome-Shaped Selectivity in LB-SPR: Length-Based Assessment of Data-Limited Inland Fish Stocks Sampled with Gillnets.” Fisheries Research 229: 105574. Hordyk, Adrian R, Kotaro Ono, Jeremy D Prince, and Carl J Walters. 2016b. “A Simple Length-Structured Model Based on Life History Ratios and Incorporating Size-Dependent Selectivity: Application to Spawning Potential Ratios for Data-Poor Stocks.” Canadian Journal of Fisheries and Aquatic Sciences 73 (12): 1787–99. https://doi.org/10.1139/cjfas-2015-0422. Mildenberger, Tobias K., Michael H. Taylor, and Matthias Wolff. 2017. “TropFishR: An r Package for Fisheries Analysis with Length-Frequency Data.” Methods in Ecology and Evolution 8 (11): 1520–27. Millar, Russell B, and Rolf Holst. 1997. “Estimation of Gillnet and Hook Selectivity Using Log-Linear Models.” ICES Journal of Marine Science 54 (3): 471–77. https://doi.org/10.1006/jmsc.1996.0194. "],["utility-functions.html", "5 Utility functions 5.1 poolLengthComp function 5.2 LoptFunc function 5.3 LcFunc function 5.4 fit_gillnet_dome function and gillnet selectivity analysis", " 5 Utility functions 5.1 poolLengthComp function The poolLengthComp function pools length data when multiple columns exist. It sums length frequency data or concatenates length composition data as a single vector. 5.2 LoptFunc function The LoptFunc function calculates optimum harvest length. Derived from Beverton 1992, in a year-class subject to a moderate and constant exponential mortality and whose individuals are growing towards an asymptotic size (as in the Von Bertalanffy growth function), the total biomass of the year-class (i.e. the product of numbers and average weight) reaches a maximum value at some intermediate age (\\(Topt\\)) at a certain weight and length of the individual fish (\\(Wopt\\) and \\(Lopt\\), respectively). \\(Lopt\\) is calculated as: \\[ Lopt = 3 \\frac {L_{\\infty}}{(3 + M/K)} \\] This function is necessary for the PoptFunc function (proportion of fish within optimal sizes in the catch). 5.3 LcFunc function The LcFunc estimates length at full selectivity. This can be done in two methods: 1) using the mode of the length-frequency distribution (LFD); or 2) applying a Kernel smoother to the LFD. For calculating the mode of the LFD, the cumulative distribution of the LFD is computed, and a loess smoother is applied to predict across equally spaced length intervals (1 cm interval); the length at which the cumulative distribution of the predictions increases the most (highest slope) corresponds to the mode of the LFD. Figure 1 shows the cumulative distribution of an example predicted LFD, and the vertical line corresponds to the mode of the distribution. Figure 5.1: Cumulative length-frequency distribution The second method is to apply a Kernel smoother to the LFD and take its maximum value as the length at full selectivity. Figure 2 shows the length at which the Kernel smoother estimates highest density estimates. Figure 5.2: Kernel smoother applied to length-frequency distribution Some results from applying these two functions to example LFDs are presented below. The left-hand plots show an example LFD, and the right-hand side plots show a subset of the example LFD, in order to test the functions with smaller sample sizes and more sparse data. Figure 5.3: Example LFD 2019 Figure 5.4: Example LFD 2018 Figure 5.5: Example LFD 2015 Figure 5.6: Example LFD 2017 These results show that for relatively complete LFDs (higher sample size, left-hand plots), the two methods (mode of LFD and kernel smoother) estimate similar lengths at full selectivity Lc. However, for more sparse length data (right-hand plots), the mode of the LFD might give very different results as for the example LFDs for 2019 and 2017. This is because the highest slope of the cumulative distribution occurred at smaller lengths. The kernel smoother seems to give more stable/reliable estimates of length at full selectivity. 5.4 fit_gillnet_dome function and gillnet selectivity analysis Gillnets retain fish based on size selectivity, where the probability of retention varies with fish length and mesh size. To quantify this selectivity, statistical models (e.g., normal or lognormal curves) are fit to catch data collected across multiple mesh sizes. Each mesh size in a gillnet has its own retention curve, modeled using a probability function. The TropFishR package implements various models using the SELECT method of Russell Millar’s selectivity equations. So the the gillnet selectivity analysis presenetd here is built upon the theoretical framework developed by Millar and Holst (1997) for gillnet selectivity analysis and the modification to that framework made by Mildenberger, Taylor, and Wolff (2017). This document describes the fit_gillnet_dome() function, which fits gillnet selectivity models using the TropFishR package. The function supports multiple dome-shaped selectivity models, and generates diagnostic plots. The fit_gillnet_dome() function is included as a standalone function in the R package fishLengthAssess. The function fits five models to length-frequency data collected from gillnets with different mesh size. The mathematics behind this involves modeling how fish of different sizes are caught by different mesh sizes, accounting for the selectivity characteristics of each mesh. The deviance residuals and goodness-of-fit statistics are used to evaluate model performance. The selectivity models include: norm.loc: Normal (common spread) model norm.sca: Normal (scaled spread) model lognorm: Lognormal model binorm.sca: Bi-normal model bilognorm: Bi-lognormal model 1. Normal Location Model (norm.loc) The normal location model assumes that the selectivity follows a normal distribution with a fixed spread across mesh sizes. Assumes selectivity curves shift with mesh size but maintain the same spread (standard deviation: \\[S(L,m) = \\exp\\left(-\\frac{(L - k \\cdot m)^2}{2\\sigma^2}\\right)\\] Where \\(S(L,m)\\) is the is the selectivity at fish length L and mesh size m, \\(k\\) is a scaling parameter for the modal length (estimated), and \\(\\sigma\\) is the standard deviation (spread of the curve).\\(k\\) and \\(\\sigma\\) are estimated parameters. 2. Normal Scale Model (norm.sca) This model also assumes a normal curve per mesh, but the spread (standard deviation) increases proportionally with mesh size. This model allows selectivity curves to have different widths depending on mesh size. \\[S(L,m) = \\exp\\left(-\\frac{(L - k_1 \\cdot m)^2}{2 \\cdot k_2 \\cdot m^2}\\right)\\] Where \\(k_1\\) determines the modal length, and \\(k_2\\) controls how the standard deviation scales with mesh size. This model allows wider selectivity curves for larger meshes.\\(k_1\\) and \\(k_2\\) are estimated parameters. 3. Lognormal Model (lognorm) The lognormal model assumes that the selectivity follows a lognormal distribution, which often provides a better fit for skewed length-frequency data. This model is commonly used when selectivity increases and decreases asymmetrically around the peak. Single peak, right-skewed. \\[S(L,m) = \\frac{m}{L} \\cdot \\exp\\left(\\mu + \\ln\\left(\\frac{m}{m_1}\\right) - \\frac{\\sigma^2}{2}\\right) \\cdot \\exp\\left(-\\frac{(\\ln(L) - \\mu - \\ln\\left(\\frac{m}{m_1}\\right))^2}{2\\sigma^2}\\right)\\] Where \\(\\mu\\) is the log-scale mean for the reference mesh size, \\(\\sigma\\) is the log-scale standard deviation, and \\(m_1\\) is the reference mesh size (typically the smallest). \\(\\mu\\) and \\(\\sigma\\) are estimated parameters. 4. Bi-normal Scale Model (binorm.sca) The bi-normal scale model combines two normal distributions, often representing different capture processes (e.g., wedging and tangling). Bi-normal distribution models two peaks in selectivity (if distinct Mode1 and Mode2), useful for species with two size classes that are selectively retained. \\[S(L,m) = p \\cdot \\exp\\left(-\\frac{(L - k_1 \\cdot m)^2}{2 \\cdot \\sigma_1 \\cdot m^2}\\right) + (1-p) \\cdot \\exp\\left(-\\frac{(L - k_2 \\cdot m)^2}{2 \\cdot \\sigma_2 \\cdot m^2}\\right)\\] Where \\(p\\) is the proportion of retention assigned to the first peak , \\(k_1, k_2\\) determine the modal lengths for each component, \\(\\sigma_1\\) and \\(\\sigma_2\\) control the standard deviations for each component. \\(p\\) is calculated as: \\[ p = \\frac{\\exp(\\theta)}{1 + \\exp(\\theta)} \\] The estimated parameters in the bi-normal scale model are: \\(k_1\\) and \\(k_2\\), \\(\\sigma_1\\) and \\(\\sigma_2\\), and \\(\\theta\\). \\(\\theta\\) is estimated in logit scale and \\(p\\) is its inverse logit, therefore the final estimate of \\(p\\) is always between 0 and 1 (a valid probability) 5. Bi-lognormal Model (bilognorm) The bi-lognormal model combines two lognormal distributions and captures two peaks (if distinct Mode1 and Mode2) but with lognormal-based selectivity functions, allowing for asymmetric selectivity at both peaks. The selectivity-at-length function for mesh size \\(m\\) is given by: \\[ S(L, m) = p \\cdot \\left( \\frac{m}{L} \\right) \\cdot \\exp\\left( \\mu_1 + \\ln\\left(\\frac{m}{m_1}\\right) - \\frac{\\sigma_1^2}{2} \\right) \\cdot \\exp\\left( -\\frac{ \\left( \\ln(L) - \\mu_1 - \\ln\\left(\\frac{m}{m_1}\\right) \\right)^2 }{2 \\sigma_1^2} \\right) + (1 - p) \\cdot \\left( \\frac{m}{L} \\right) \\cdot \\exp\\left( \\mu_2 + \\ln\\left(\\frac{m}{m_1}\\right) - \\frac{\\sigma_2^2}{2} \\right) \\cdot \\exp\\left( -\\frac{ \\left( \\ln(L) - \\mu_2 - \\ln\\left(\\frac{m}{m_1}\\right) \\right)^2 }{2 \\sigma_2^2} \\right) \\] Where \\(m_1\\) is the reference mesh size (usually the smallest in the set), \\(\\mu_1\\) and \\(\\mu_2\\) are the log-scale location parameters for the two domes, \\(\\sigma_1\\) and \\(\\sigma_2\\) are the log-scale spread parameters for the two domes, and \\(p\\) is the proportion of fish following the first dome (proportion of retention assigned to the first peak), estimated as: \\[ p = \\frac{\\exp(\\theta_5)}{1 + \\exp(\\theta_5)} \\] The estimated parameters in the bi-lognormal model are: \\(\\mu_1\\), \\(\\mu_2\\), \\(\\sigma_1\\) and \\(\\sigma_2\\) in log-scale. \\(\\theta\\) is estimated in logit scale and \\(p\\) is its inverse logit, therefore the final estimate of \\(p\\) is always between 0 and 1 (a valid probability). 5.4.1 Function Overview The fit_gillnet_dome is part of the fishLengthAssess R package and the function contains the following key features: The function allows the user to decide whether to fit all five models (i.e., norm.loc, norm.sca, lognorm, binorm.sca and bilognorm) or to omit the bimodal models (binorm.sca and bilognorm), using the arument run_bimodal=FALSE/TRUE. By default, the bimodal models are omitted (FALSE). Generates selectivity curves and residual plots for each model. Provides statistics to compare model performance. It automatically calculates appropriate starting values for complex selectivity models (e.g., binorm.sca and bilognorm) based on the length distribution of data. Also, it allows the user to use pre-defined starting values when automatic calculation for binorm.sca and bilognorm is suboptimal. Creates diagnostic plots to understand length distributions across mesh sizes. The fit_gillnet_dome() function streamlines the workflow of gillnet selectivity analysis through three main steps: Exploratory analysis and automatic starting value estimation Model fitting for multiple selectivity curves Result visualization and comparative statistics fit_gillnet_dome &lt;- function(input_data, mesh_sizes, run_bimodal=FALSE, manual_x0_list = list(), length_seq = seq(40, 100, 0.1), output_dir = &quot;model_plots&quot;, criterion = &quot;Deviance&quot;, sd_spread = 7, rel.power = NULL, verbose = TRUE) 5.4.2 Function Arguments Arguments Description input_data Data frame with fish lengths in first column and catches for each mesh size in subsequent columns mesh_sizes Vector of mesh sizes in the same order as the columns in input_data run_bimodal Logical (TRUE/FALSE) that controls whether bimodal models (binorm.sca and bilognorm) are fitted. Default: FALSE manual_x0_list Optional list of manually specified starting values for each model type length_seq Sequence of length values for plotting selectivity curves (default: 40-100 cm by 0.1) output_dir Directory where plots will be saved (default: “model_plots”) criterion Criterion for model selection (“Deviance” or “LogLikelihood”) sd_spread Range around detected modes used to calculate starting values for standard deviations (default: 7) rel.power Optional vector of relative fishing powers for each mesh size verbose Whether to print progress messages (default: TRUE) 5.4.3 Data exploration and calculation of starting values The first step of the function performs exploratory analysis to understand the catch distribution across length classes and mesh sizes. It then uses these insights to automatically calculate appropriate starting values for the optimization process. Data visualization Four key plots are generated and saved in the specified output directory: Length distribution by mesh size: Histograms showing catch at each length class for each mesh. Catch distribution across length classes: Line plots comparing catch patterns between mesh sizes. Aggregated length distribution: Bar plot of total catch by length class across all meshes. Detected peaks: Visualization of the identified modes in the length frequency data. Figure 5.7: Exploratory visualization of data. The above plot shows the four exploratory visualizations combined. The first panel (top left) shows the length distribution by mesh size, the second panel (top right) shows the catch distribution across length classes, the third panel (bottom left) shows the aggregated length distribution, and the fourth panel (bottom right) shows the detected peaks (Mode1 in red, Mode2 in blue). Calculation of starting values for binorm.sca and bilognorm An important feature of this function is that it only calculates and provides starting values (x0) for the more complex bimodal models (binorm.sca and bilognorm). For the simpler models (norm.loc, norm.sca, and lognorm), x0 is intentionally set to NULL. This is because: The select_Millar() function internally calls gillnetfit() for these simpler models, which has its own robust mechanism for estimating initial values. Simpler models are less sensitive to starting values and generally converge well without external initialization. Bimodal models have more parameters (including the proportion parameter p) and are more prone to convergence issues without good starting values. The function uses the findpeaks() function from the pracma package to detect modes in the aggregated length distribution: peaks &lt;- findpeaks(total_counts, nups = 2, ndowns = 2, minpeakheight = max(total_counts) * 0.1) nups = 2: Requires at least 2 increasing points before the peak ndowns = 2: Requires at least 2 decreasing points after the peak minpeakheight = max(total_counts) * 0.1: Ignores small peaks (less than 10% of maximum) Starting values calculation The function handles three scenarios: Two or more peaks detected: Mode1 = location of first peak Mode2 = location of second peak Only one peak detected: Mode1 = location of the peak Mode2 = location of highest catch at least 5 cm away from Mode1, or 75th percentile if no suitable secondary peak No clear peaks: Mode1 = 30th percentile of length distribution Mode2 = 70th percentile of length distribution Standard deviations are estimated from the data in windows around each mode: #It keeps all lengths within a window of +/- sd_spread cm around Mode1 and Mode2. #e.g. if Mode1 = 40 and sd_spread = 7, it keeps lengths from 33 to 47 cm subset1 &lt;- plot_data$MidLength[plot_data$MidLength &gt; (Mode1 - sd_spread) &amp; plot_data$MidLength &lt; (Mode1 + sd_spread)] #selects a subset of fish lengths around Mode1 subset2 &lt;- plot_data$MidLength[plot_data$MidLength &gt; (Mode2 - sd_spread) &amp; plot_data$MidLength &lt; (Mode2 + sd_spread)] #selects a subset of fish lengths around Mode2 #Now if If the SD is missing (NA), or too small (less than 3 cm) (too narrow curve). Then it defaults to 3.5 cm as a safe minimum value StdDev1 &lt;- ifelse(is.na(sd(subset1)) || sd(subset1) &lt; 3, 3.5, sd(subset1)) StdDev2 &lt;- ifelse(is.na(sd(subset2)) || sd(subset2) &lt; 3, 4.5, sd(subset2)) For bimodal models, the function also calculates the proportion parameter based on the relative catch in each mode: Catch_Mode1 &lt;- sum(total_counts[plot_data$MidLength &gt; (Mode1 - sd_spread) &amp; plot_data$MidLength &lt; (Mode1 + sd_spread)], na.rm = TRUE) Catch_Mode2 &lt;- sum(total_counts[plot_data$MidLength &gt; (Mode2 - sd_spread) &amp; plot_data$MidLength &lt; (Mode2 + sd_spread)], na.rm = TRUE) P_Mode1 &lt;- min(max(Catch_Mode1 / (Catch_Mode1 + Catch_Mode2), 0.75), 0.95) P_Mode1_logit &lt;- qlogis(P_Mode1) # Convert to logit scale For lognormal-based models, the function converts raw-space parameters to log-space parameters: log_sd_from_raw &lt;- function(mean_val, sd_val) sqrt(log(1 + (sd_val / mean_val)^2)) LogStdDev1 &lt;- min(max(log_sd_from_raw(Mode1, StdDev1), 0.1), 0.4) LogStdDev2 &lt;- min(max(log_sd_from_raw(Mode2, StdDev2), 0.1), 0.4) This transformation uses the mathematical relationship between normal and lognormal distributions: If \\(X \\sim \\text{LogNormal}(\\mu, \\sigma^2)\\), then: - Mode of \\(X = e^{\\mu - \\sigma^2}\\) - Variance of \\(X = (e^{\\sigma^2} - 1) \\cdot e^{2\\mu + \\sigma^2}\\) Solving for \\(\\sigma\\) given the mode and standard deviation: \\(\\sigma = \\sqrt{\\log\\left(1 + \\left(\\frac{\\text{StdDev}}{\\text{Mode}}\\right)^2\\right)}\\) 5.4.4 Model Fitting This step fits multiple selectivity models using the automatically calculated starting values from the previous step or using starting values manually provided if specified. The run_bimodal parameter controls which models are fitted: When run_bimodal = TRUE (default): All five selectivity models are fitted (three unimodal and two bimodal models) When run_bimodal = FALSE: Only the three unimodal models are fitted. Category Models Description Unimodal norm.loc, norm.sca, lognorm Always fitted regardless of run_bimodal setting Bimodal binorm.sca, bilognorm Only fitted when run_bimodal = TRUE The function fit_gillnet_dome prepares the data in the format required by the select_Millar() function: data_list &lt;- list( midLengths = midLengths, meshSizes = mesh_sizes, CatchPerNet_mat = CatchPerNet_mat, rel.power = rel.power ) In the model fitting loop, three or five models are fitted: Normal location (norm.loc) Normal scale (norm.sca) Lognormal (lognorm) Bi-normal scale (binorm.sca) (optional) Bi-lognormal (bilognorm) (optional) Each model in the selected set is fitted sequentially using an error-handling approach: for (model in models) { cat(&quot;\\nFitting model:&quot;, model, &quot;...\\n&quot;) # Determine starting values for each model x0 &lt;- if (model %in% names(full_x0_list)) full_x0_list[[model]] else NULL # Try to fit the model, catch any errors results[[model]] &lt;- tryCatch({ select_Millar(data_list, x0 = x0, rtype = model, rel.power = rel.power, plot = FALSE) }, error = function(e) { cat(&quot;Error fitting model:&quot;, model, &quot;- Skipping.\\n&quot;) return(NULL) }) } results &lt;- results[!sapply(results, is.null)] if (length(results) == 0) { cat(&quot;No models fitted successfully.\\n&quot;) return(NULL) } This loop iterates through each model type (unimodal and bimodal if selected),provides informative output to track progress during model fitting, and determines appropriate starting values: For bimodal models: Uses the automatically detected peaks or manually provided values. For unimodal models: Allows the select_Millar() function to calculate starting values internally. Also, the loops contains a tryCatch() to handling errors: If a model successfully fits: Stores the result in the results list. If a model fails to converge or encounters numerical issues: Captures the error, logs a message, and continues with the next model. This prevents a single problematic model from causing the entire analysis to fail. After the loop completes, the function filters out any failed models before proceeding to model comparison and visualization steps. 5.4.5 Model parameter summary After fitting, a summary table is created with key parameters from each model: summary_table &lt;- data.frame( Model = names(results), LogLikelihood = sapply(results, function(x) x$out[&quot;model.l&quot;, 1]), Deviance = sapply(results, function(x) x$out[&quot;Deviance&quot;, 1]), Mode1 = sapply(results, function(x) x$estimates[1, &quot;par&quot;]), StdDev1 = sapply(results, function(x) x$estimates[2, &quot;par&quot;]), Mode2 = sapply(results, function(x) ifelse(nrow(x$estimates) &gt; 2, x$estimates[3, &quot;par&quot;], NA)), StdDev2 = sapply(results, function(x) ifelse(nrow(x$estimates) &gt; 3, x$estimates[4, &quot;par&quot;], NA)), P_Mode1 = sapply(results, function(x) ifelse(nrow(x$estimates) &gt; 4, x$estimates[5, &quot;par&quot;], NA)) ) Models are sorted by Deviance and Log-likelihood values. Log-Likelihood and Deviance are statistical measures used to evaluate the goodness of fit of different models. The Log-Likelihood value indicates how well a model fits the observed data. Higher values are better (closer to zero, as log-likelihoods are negative). When comparing models with the same number of parameters, the one with the higher log-likelihood provides a better fit. Deviance is derived from the log-likelihood and measures the departure of the model from a perfectly fitting model. It allows for formal statistical tests between nested models. The model with the lowest deviance provides the closest fit to the observed data. Based on the Log-likelihood values, the user can calculate AIC (AIC = -2 × log-likelihood + 2 × k), where k is the number of parameters. The model with the lowest AIC is considered the best balance between goodness-of-fit and parsimony. The following table shows an example of a statistical summary. Table 5.1: Example summary table showing parameter estimates and fit statistics for different models (only unimodal models) Model LogLikelihood Deviance Mode1 StdDev1 lognorm 25014.23 704.28 54.91 4.46 norm.sca 24979.99 772.76 55.28 4.34 norm.loc 24934.91 862.92 54.60 5.15 5.4.6 Result visualization and interpretation The final step creates visual outputs for each successfully fitted model. Selectivity curves For each model, selectivity curves are calculated for all mesh sizes across the specified length range: rmatrix &lt;- outer(plotlens, meshSizes, rtypes_Millar(res$rtype), res$par) rmatrix &lt;- t(t(rmatrix) * res$rel.power) These curves show the relative retention probability for fish of different lengths in each mesh size. Deviance Residuals Deviance residuals help assess model fit by showing where the model predictions differ from observed data: dev_res_df &lt;- data.frame( Length = rep(res$midLengths, times = nmeshes), MeshSize = factor(rep(meshSizes, each = length(res$midLengths))), Residuals = as.vector(res$Dev.resids) ) Bubble plots visualize these residuals, with: - Bubble size proportional to residual magnitude - Blue bubbles for negative residuals (model overprediction) - Red bubbles for positive residuals (model underprediction) Output plots For each model, the function creates and saves: 1. A selectivity curve plot showing relative retention by length for each mesh size 2. A deviance residual bubble plot showing fit quality across lengths and mesh sizes 3. Combined plots with both visualizations together Below are two examples of plots for two different selectivity models: Figure 5.8: Normal location model (norm.loc) selectivity curves and residuals. The top panel shows the selectivity curves for each mesh size, while the bottom panel displays the deviance residuals. Figure 5.9: Lognormal model (lognorm) selectivity curves and residuals. The top panel shows the selectivity curves for each mesh size, while the bottom panel displays the deviance residuals. Each plot shows: Selectivity curve plot (top panel): X-axis: Fish length (cm) Y-axis: Relative retention (scaled to maximum of 1) Lines: Each colored line represents a different mesh size Interpretation: Each curve shows the relative probability of catching fish of different lengths with a specific mesh size The peak of each curve indicates the optimal fish length for that mesh size Curves typically shift to the right as mesh size increases The width of curves indicates the selectivity range Comparing curve shapes across models helps evaluate which provides the most realistic representation Deviance residuals bubble plot (bottom panel) X-axis: Fish length (cm) Y-axis: Mesh size (cm) Bubbles: Size: Indicates the absolute magnitude of the deviance residual Color: Blue for negative residuals (model overestimates), red for positive residuals (model underestimates) Position: Each bubble positioned at specific length × mesh size combination Interpretation: Good-fitting models have smaller bubbles distributed randomly Systematic patterns (e.g., clusters of same-colored bubbles) indicate areas where the model fits poorly Areas with large bubbles indicate specific length-mesh combinations where the model predictions differ substantially from observed data These visualizations are saved for each model in the specified output directory, providing a comprehensive visual assessment of model performance. References Mildenberger, Tobias K., Michael H. Taylor, and Matthias Wolff. 2017. “TropFishR: An r Package for Fisheries Analysis with Length-Frequency Data.” Methods in Ecology and Evolution 8 (11): 1520–27. Millar, Russell B, and Rolf Holst. 1997. “Estimation of Gillnet and Hook Selectivity Using Log-Linear Models.” ICES Journal of Marine Science 54 (3): 471–77. https://doi.org/10.1006/jmsc.1996.0194. "],["references.html", "References", " References Beverton, R. J. H. 1992. “Patterns of Reproductive Strategy Parameters in Some Marine Teleost Fishes.” Journal of Fish Biology 41 (sB): 137–60. https://doi.org/10.1111/j.1095-8649.1992.tb03875.x. Coscino, Connor L., Lyall Bellquist, William J. Harford, and Brice X. Semmens. 2024. “Influence of Life History Characteristics on Data-Limited Stock Status Assertions and Minimum Size Limit Evaluations Using Length-Based Spawning Potential Ratio (LBSPR).” Fisheries Research 276 (August): 107036. https://doi.org/10.1016/j.fishres.2024.107036. Froese, Rainer. 2004. “Keep It Simple: Three Indicators to Deal with Overfishing.” Fish and Fisheries 5 (1): 86–91. https://doi.org/10.1111/j.1467-2979.2004.00144.x. Harford, W. 2024. “fishSimGTG: Numerical Simulations of Fish Population Dynamics. R Package Version 1.0.6.” https://github.com/natureanalytics-ca/fishSimGTG. Hommik, Kristjan, Ciaran Fitzgerald, Finbarr Kelly, and Samuel Shephard. 2020. “Dome-Shaped Selectivity in LB-SPR: Length-Based Assessment of Data-Limited Inland Fish Stocks Sampled with Gillnets.” Fisheries Research 229: 105574. Hordyk, A. 2021. “LBSPR: Length-Based Spawning Potential Ratio. R Package Version 0.1.6.” https://github.com/AdrianHordyk/LBSPR. Hordyk, Adrian R., Kotaro Ono, Jeremy D. Prince, and Carl J. Walters. 2016a. “A Simple Length-Structured Model Based on Life History Ratios and Incorporating Size-Dependent Selectivity: Application to Spawning Potential Ratios for Data-Poor Stocks.” Canadian Journal of Fisheries and Aquatic Sciences 73 (12): 1787–99. https://doi.org/10.1139/cjfas-2015-0422. Hordyk, Adrian R, Kotaro Ono, Jeremy D Prince, and Carl J Walters. 2016b. “A Simple Length-Structured Model Based on Life History Ratios and Incorporating Size-Dependent Selectivity: Application to Spawning Potential Ratios for Data-Poor Stocks.” Canadian Journal of Fisheries and Aquatic Sciences 73 (12): 1787–99. https://doi.org/10.1139/cjfas-2015-0422. Mildenberger, Tobias K., Michael H. Taylor, and Matthias Wolff. 2017. “TropFishR: An r Package for Fisheries Analysis with Length-Frequency Data.” Methods in Ecology and Evolution 8 (11): 1520–27. Millar, Russell B, and Rolf Holst. 1997. “Estimation of Gillnet and Hook Selectivity Using Log-Linear Models.” ICES Journal of Marine Science 54 (3): 471–77. https://doi.org/10.1006/jmsc.1996.0194. "]]
